"""Summary evaluator runner."""

import logging
import re
from typing import Dict, List, Union

import attrs

from medbench.datasets import (
    CORRECT_TAG,
    Data,
    Dataset,
    EMediaObjectType,
    Instance,
    MediaObject,
    Reference,
)
from medbench.models import (
    ModelOutput,
    ModelRegistry,
    ModelRun,
    Runner,
    SystemPromptModel,
)

from .multimodal import MultimodalEvaluatorRunner


@attrs.define(kw_only=True)
class SummaryEvaluatorRunner(MultimodalEvaluatorRunner):
    """Evaluator runner for summarization tasks.

    This evaluator evaluates summaries based on its groundness and
    thoroughness, by rating how well the summary answers a set of questions
    generated from the original input.

    This approach assumes the evaluator model can:
    - Identify and prioritize important information in the original input;
    - Answer questions strictly from some context information (summary), even
    if the information is not correct.

    ## Evaluation process:
    1. Based on the original input, generate questions that should be answered by a thorough summary.
        - Output is expected to contain the question, a rating of its importance, and the expected answer.
    2. Answer the questions with the summary generated by the model.
        - Output is expected to contain the answer, a rating of its completeness, and potentially missing information.
    3. Feed questions and answers to the evaluator model to rate the summary (final scoring).

    ## Customization

    The evaluation process can be customized by modifying the focus of the
    evaluator model when generating questions from the original input. For example,
    You may instruct the evaluator to focus on facts and data correctness, or
    to focus on the correlation between different pieces of information.

    When modifying the question or answer generation prompts, make sure to maintain
    the triplet output format, or modify the `_process_triplet_output` method accordingly.
    """

    questions_generator: SystemPromptModel = attrs.field(init=False)
    questions_generator_system_prompt: str = """\
You are an AI assistant with deep expertise in the medical domain. \
Your goa is to generate questions that you expect to \
be answered by a thorough summary of a given input.

You will be given a piece of text and your task is to:
- Generate questions that you expect to be answered by a complete and correct summary of the text;
- Rate the importance of each question with a score between 1 and 3;
- Answer each question generated with a concise and direct answer.

The questions, ratings and answers you give will be used to evaluate the performance of \
other AI systems in generating summaries for the same input you were given.

At this point we are not concerned about the correctness of the content, we rather want \
a truthful representation of the input, so refrain from using your own knowledge for \
correcting the information or any of the answers.

For the rating, put yourself in the shoes of a medical professional and \
think whether that information is necessary for them to conduct their work. \
A rating of 1 means the information is a good to have, but not strictly necessary. \
And a rating of 3 means the information is crucial for a complete summary.

All questions, ratings and answers you generate from the original input will be \
concise and direct. And considering your output \
be programmatically processed, you will stricly follow the template between the backticks:
```
question [rating][answer]
```

- Each line of your output must contain all three elements of the triplet.
- Replace question with the question, rating with the score, and answer with the answer.
- The rating and the answer must always be inside square brackets.

Generate as many questions as you deem necessary to be thorough and cover the entire input.\
"""
    questions_generator_runner: Runner = None

    answerer: SystemPromptModel = attrs.field(init=False)
    answerer_system_prompt: str = """\
You are an AI assistant with deep medical expertise, \
and your goal is to judge whether a piece of text can answer a set of questions.

You are part of a team of AI assistants that are evaluating the performance of \
other AI systems in generating summaries for medical related tasks. In this case, \
another AI assistant has already generated a list of questions that are expected to be answered \
by a thorough summary of the original input.

You will be given the generated questions along with the summary generated by the AI system being evaluated, \
and your tasks are to:
- Answer each of the questions using only information presented in the summary
- Rate the completeness of the answer with a score between 1 and 5;
- Related to each question, provide what additional information you expected to \
see in the summary that would result in a higher rate.

At this point we are not concerned about the correctness of the answer, we rather want \
a truthful representation of the summary, so refrain from using your own knowledge for \
correcting the summary or any of the answers.

A rating of 1 means the questions is not answered at all. \
And a rating of 5 means the question is answered completely, already \
including additional details that can be useful.

All answers, ratings and details you give will be \
concise and direct. And considering your output \
be programmatically processed, you will stricly follow the template between the backticks:

```
answer [rating][extra details]
```

- Answers the questions in the same order they were given to you.
- Each line of your output must contain all three elements of the triplet.
- Replace answer with the generated answer, rating with the score and extra details with \
the additional information you expected to see.
- You may leave extra details empty, but be sure to include the square brackets.
- If no answer to the question is given, say so and give a rating of 1.
"""
    answerer_runner: Runner = None

    system_prompt: str = """\
{base_eval_prompt}

This time you will evaluate AI systems responses on the {dataset_name} dataset. \
Below is dataset description:
{dataset_description}
{task_specific_eval}
{output_specs_prompt}
"""
    base_eval_prompt: str = """\
You are an AI assistant with deep expertise in the medical domain. \
Your task is to evaluate the quality of summaries produced by other AI systems.

To measure the quality of the summary you will be given:
- Original input that needs to be summarized;
- Questions that are expected to be answered by a thorough summary;
- Answers extracted from the original input (ground truth);
- Ratings judging the importance of the question to the summary (1 is lowest, 3 is highest).
- Answers extracted from the summary generated by the AI system being evaluated;
- Extra information that if included would improve the quality of answers extracted from the summary;
- Ratings judging the quality of the answer extracted from the summary (1 is lowest, 5 is highest).

You will measure quality according to the following criteria:
- Groundness: The summary must faithfully represent the original input, independently of the original input being correct or not;
- Completeness: The summary must cover all necessary aspects of the medical query or context;
- Relevance: The summary must be a summary of the original input, and important information needs to be prioritized;
- Fluency: The summary must be coherent and well-structured, extra points for connecting information;

Note how correctness is a secondary concern. Consistency and faithfulness to the original input take precedence over the correctness of the information. \
Thus, hallucinations and omissions are more important findings. In any case, you must still evaluate correctness of inferences made in the summary, by, \
for example, connecting information from the original input.

Ultimately, you should put yourself in the shoes of a medical professional and evaluate the response as if it was given by a human expert. \
You shall judge whether the response, if given in a real case scenario, would be helpful to you (a medical professional) to conduct your work \
in the best way possible.\
"""
    output_specs_prompt: str = """\
Following the evaluation guidelines, you must score each response with a whole number between 1 and 5, where:
- 1: The summary does not correctly prioritizes information, is hard to read, and includes information not explicitly given in the original input.
- 2: The summary includes the most important information, but also includes hallucinated or many irrelevant facts. It may be complex or hard to read.
- 3: The summary strikes a balance between relevant information, but still lacks on completeness and groundness. Readability is good.
- 4: The summary includes most important information with no hallucinated facts. It could include extra information or be more on point.
- 5: The summary is complete, including additional information. It adds connections to the existing data while maintaining groundness, relevancy and correctness.

Before scoring the response, think step by step about the evaluation guidelines and the task requirements. \
Revisit all evaluation criteria and be direct yet specific about the reasons for your score.

At the very end, you must present only the final score. Refrain from making any other remarks at this point. \
All in all your response shall match the following template:
```
Explanation:
All explanation and analysis of evaluation criteria must come here.

Score: Final score between 1 and 5.
```\

Your output is programmatically processed, so make sure to follow the template exactly.\
"""
    task_specific_eval_prompt: str = ""

    skip_errors: bool = False

    def __attrs_post_init__(self):
        super().__attrs_post_init__()

        self.questions_generator = self.evaluator.evolve(
            system_prompt=self.questions_generator_system_prompt
        )

        self.answerer = self.evaluator.evolve(system_prompt=self.answerer_system_prompt)

        if self.questions_generator_runner is None:
            logging.info("Initializing questions generator runner.")
            questions_generator_id = ModelRegistry.get_registered_name(
                type(self.questions_generator)
            )
            QuestionsGeneratorRunner = ModelRegistry.get_runner(questions_generator_id)
            self.questions_generator_runner = QuestionsGeneratorRunner(is_eval=True)

        if self.answerer_runner is None:
            logging.info("Initializing answerer runner.")
            answerer_id = ModelRegistry.get_registered_name(type(self.answerer))
            AnswererRunner = ModelRegistry.get_runner(answerer_id)
            self.answerer_runner = AnswererRunner(is_eval=True)

        # Validate that (if passed) the questions_generator_runner
        # has questions for all instances of the dataset
        if (
            self.questions_generator_runner
            and self.questions_generator_runner._model_run
        ):
            if not self.questions_generator_runner._model_run.results:
                raise ValueError(
                    "Questions generator model run has no results. "
                    "Please generate questions for all instances or "
                    "do not give a questions_generator_runner."
                )
            for instance, model_output, control_instance in zip(
                self.questions_generator_runner._model_run.dataset.instances,
                self.questions_generator_runner._model_run.results,
                self.predictions_model_run.dataset.instances,
            ):
                if not self.skip_errors and model_output.error is not None:
                    raise ValueError(
                        "Questions generator model run has errors. "
                        "Please generate questions for missing instances."
                    )

                if instance.id != control_instance.id:
                    raise ValueError(
                        "Mismatch found between precitions' dataset and "
                        "questions generator's input."
                    )

    async def evaluate(self) -> None:
        """Run summarization evaluator workflow.

        1. Given the dataset input, generate questions that should be answered by a thorough summary.
            - We could also rate the questions based on how necessary they are. Afterall, a short and concise summary can also be good.
        2. Answer questions only with the summary generated by the model.
        3. Evaluate the answers based on the original input (groundness).
            - Output a score between 1 and 5.
        """
        logging.info("Running summarization evaluator workflow.")

        if (
            self.questions_generator_runner._model_run is None
            or not self.questions_generator_runner._model_run.results
        ):
            logging.debug("Generating questions from dataset input.")
            self.questions_generator_runner.setup(
                ModelRun(
                    id=f"{self.predictions_model_run.id}-questions",
                    model=self.questions_generator,
                    dataset=self.predictions_model_run.dataset,
                )
            )
            self.questions_generator_runner.run()
        else:
            logging.debug("Questions already generated. Skipping.")

        questions: List[Instance] = self._process_triplet_output(
            self.questions_generator_runner._model_run.results,
        )
        logging.debug(f"Generated a total of {len(questions)} questions.")

        # TODO: Fix this. Now, _prepare_evaluation_instances is only taking the first question.
        # we should iterate over inputs, and WHILE questions have the same id, append them to the same instance.
        # `questions` should still have
        logging.debug("Generating answers from summary.")
        self.answerer_runner.setup(
            ModelRun(
                id=f"{self.predictions_model_run.id}-answers",
                model=self.answerer,
                dataset=Dataset(
                    name=f"{self.predictions_model_run.dataset.name}-answers",
                    description="Answers of questions the summary should be able to answer.",
                    instances=self._prepare_summary_questions_instances(
                        questions, self.predictions_model_run.results
                    ),
                ),
            )
        )
        self.answerer_runner.run()

        answers: List[Instance] = self._process_triplet_output(
            self.answerer_runner._model_run.results,
            extra_information_metadata_key="extra_information",
        )
        logging.debug(f"Generated a total of {len(answers)} answers.")

        logging.debug("Evaluating summary.")
        self.evaluator_runner.setup(
            ModelRun(
                id=f"{self.predictions_model_run.id}-evaluation",
                model=self.evaluator,
                dataset=Dataset(
                    name=f"{self.predictions_model_run.dataset.name}-evaluation",
                    description="Evaluation of the summary generated by the AI system.",
                    instances=self._prepare_summary_evaluation_instances(
                        dataset_inputs=self.predictions_model_run.dataset.instances,
                        summaries=self.predictions_model_run.results,
                        questions=questions,
                        answers=answers,
                    ),
                ),
            )
        )
        self.evaluator_runner.run()

    def _process_triplet_output(
        self,
        results: List[ModelOutput],
        extra_information_metadata_key: Union[str | None] = None,
    ) -> List[Instance]:
        """Process the output of the questions generator model.

        Expected string format:
        ```
        main information [rating][extra information]
        multiple lines are possible [rating][extra information]
        empty extra informations are also possible [rating][]
        ```

        Args:
            results (List[ModelOutput]): List of model outputs.
            extra_information_metadata_key (str): Optional key to store extra information in the metadata.
                If None, extra information is stored in the references.

        Returns:
            List[Instance]: List of instances.
        """
        instances = []
        for model_output in results:
            if model_output.error is not None:
                logging.warning(
                    f"Skipping instance {model_output.input_id} due to error: {model_output.error}"
                )
                continue

            contents = model_output.completions.get_text()
            for line in contents.strip().split("\n"):
                if line.strip() == "":
                    continue

                # Match question rating and expected answer
                match = re.match(r"(.*)\[(\d)\]\[(.*)\]", line)
                if not match:
                    if self.skip_errors:
                        continue
                    else:
                        raise ValueError(
                            f"Error while processing triplets output. Invalid content format: {line}"
                        )

                main_text, rating, extra_information = match.groups()
                rating = int(rating)
                extra_information = extra_information.strip()

                references = []
                metadata = {"rating": rating}

                if extra_information_metadata_key is None:
                    references.append(
                        Reference(
                            output=Data.from_text(data=extra_information),
                            tags=[CORRECT_TAG],
                        )
                    )
                else:
                    metadata[extra_information_metadata_key] = extra_information

                instances.append(
                    Instance(
                        id=model_output.input_id,
                        input=Data(
                            content=[
                                MediaObject(type=EMediaObjectType.TEXT, data=main_text)
                            ]
                        ),
                        references=references,
                        split="eval",
                        metadata=metadata,
                    )
                )

        return instances

    def _prepare_summary_questions_instances(
        self,
        questions: List[Instance],
        summaries: List[ModelOutput],
        questions_header: str = "QUESTIONS:\n\n",
        summaries_header: str = "\n\nAI SYSTEM SUMMARY:\n\n",
    ) -> List[Instance]:
        """Prepare instances to answer questions with summary.

        Args:
            questions (list of Instance): Questions about the original input.
                The `id` of each question should match with the id of a summary.
            summaries (list of ModelOutput): Output being evaluated.
            questions_header (str, optional): Header to add before the questions.
            summaries_header (str, optional): Header to add before the summaries.

        Returns:
            list of Instance: Prepared instances that can be used to
                generate answers based on the summary.
        """
        summary_questions_map: Dict[str, List[Instance]] = {}
        for question in questions:
            if question.id not in summary_questions_map:
                summary_questions_map[question.id] = []
            summary_questions_map[question.id].append(question)

        prepared_instances: List[Instance] = []
        for summary in summaries:
            if summary.error is not None:
                logging.debug(
                    "Cannot prepare summary questions for evaluation. "
                    f"Skipping instance {summary.input_id} due to error: {summary.error}"
                )
                continue

            if summary.input_id not in summary_questions_map:
                logging.debug(
                    f"Skipping instance {summary.input_id} because no questions were generated."
                )
                continue

            # Questions and references are in the same format, one line per entry.
            questions_text = "\n".join(
                [
                    q.input.get_text().strip()
                    for q in summary_questions_map[summary.input_id]
                ]
            )
            questions_references = "\n".join(
                [
                    r.output.get_text().strip()
                    for q in summary_questions_map[summary.input_id]
                    for r in q.references
                ]
            )
            prepared_instances.append(
                Instance(
                    id=summary.input_id,
                    input=Data.from_text(
                        data=(
                            questions_header
                            + questions_text
                            + summaries_header
                            + summary.completions.get_text()
                        )
                    ),
                    references=questions_references,
                    split="eval",
                )
            )

        return prepared_instances

    def _prepare_summary_evaluation_instances(
        self,
        dataset_inputs: List[Instance],
        summaries: List[ModelOutput],
        questions: List[Instance],
        answers: List[Instance],
    ) -> List[Instance]:
        # TODO Build a dict mapping id to the fields (question, answer, rating, extra_information)
        # which in turn have lists with all instances.
        # Maybe tuples are better than mapping to fields (dict of dicts)

        # TODO: Filter should actually take into account id of instance, and id of questions
        # i.e.: we should know wheter all questions, or just some were answered.

        # Filter out questions, inputs and summaries we couldn't generate answers for
        answers_ids = {a.id for a in answers}
        questions = [q for q in questions if q.id in answers_ids]
        filtered_inputs = [i for i in dataset_inputs if i.id in answers_ids]
        filtered_summaries = [s for s in summaries if s.input_id in answers_ids]

        evaluation_instances = []
        for original_input, summary in zip(filtered_inputs, filtered_summaries):
            questions_answers = []
            references = []
            for question, answer in zip(questions, answers):
                if question.metadata["rating"] < 3:
                    continue
                questions_answers.append(
                    (
                        f"\t- QUESTION: {question.input.get_text()}\n"
                        f"\t  IMPORTANCE RATING: {question.metadata['rating']}\n"
                        f"\t  ANSWER FROM SUMMARY: {answer}\n"
                        f"\t  ANSWER COMPLETENESS RATING: {answer.metadata['rating']}\n"
                        f"\t  POTENTIALLY MISSING INFORMATION: {answer.metadata.get('extra_information', '')}\n"
                        f"\t  EXPECTED ANSWER (FROM ORIGINAL INPUT): {question.references[0].output.get_text()}"
                    )
                )
                references += question.references

            questions_answers_str = "\n".join(questions_answers)
            evaluation_instances.append(
                Instance(
                    id=original_input.id,
                    input=Data.from_text(
                        data=(
                            "ORIGINAL INPUT:\n\n"
                            f"{original_input.input.get_text()}\n\n"
                            "QUESTIONS SUMMARY SHOULD ANSWER:\n\n"
                            f"{questions_answers_str}\n\n"
                            "AI SYSTEM GENERATED SUMMARY:\n\n"
                            f"{summary.completions.get_text()}"
                        )
                    ),
                    references=references,
                    split=original_input.split,
                )
            )

        return evaluation_instances
