{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM as an evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current learnings:\n",
    "- Limitations on TPM\n",
    "- Limitations on output length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from medbench.config import settings\n",
    "from medbench.datasets import Dataset\n",
    "from medbench.evaluators import MultimodalEvaluatorRunner, SummaryEvaluatorRunner\n",
    "from medbench.models import ModelOutput, ModelRun\n",
    "from medbench.models.azureoai import OpenAIChatModel, OpenAIReasoningModel\n",
    "from medbench.models.cxrreportgen import CXRReportGenModel\n",
    "from medbench.utils import load_arena_data\n",
    "\n",
    "load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmed_gpt4o_path = (\n",
    "    \"../data/arena/segmed/segmed/gpt-4o.json\"\n",
    ")\n",
    "segmed_cxrreportgen_path = (\n",
    "    \"../data/arena/segmed/segmed/cxrreportgen.json\"\n",
    ")\n",
    "segmed_dataset_path = (\n",
    "    \"../data/arena/segmed/segmed/data.json\"\n",
    ")\n",
    "\n",
    "with open(segmed_dataset_path, \"r\") as f:\n",
    "    segmed_dataset_json = json.load(f)\n",
    "    segmed_dataset_json[\"description\"] = (\n",
    "        \"Image to findings dataset. Given Chest X-ray images, the model predicts the findings.\"\n",
    "    )\n",
    "    segmed_dataset = Dataset.from_json(segmed_dataset_json)\n",
    "\n",
    "with open(segmed_gpt4o_path, \"r\") as f:\n",
    "    segmed_gpt4o_json = json.load(f)\n",
    "    segmed_gpt4o_model_run = ModelRun(\n",
    "        id=\"gpt-4o-sampled_segmed_gpt4\",\n",
    "        model=OpenAIChatModel.from_json(segmed_gpt4o_json[\"model\"]),\n",
    "        dataset=segmed_dataset,\n",
    "        results=[ModelOutput.from_json(o) for o in segmed_gpt4o_json[\"results\"]],\n",
    "    )\n",
    "\n",
    "with open(segmed_cxrreportgen_path, \"r\") as f:\n",
    "    segmed_cxrreportgen_json = json.load(f)\n",
    "    segmed_cxrreportgen_model_run = ModelRun(\n",
    "        id=\"cxrreportgen-sampled_segmed_gpt4\",\n",
    "        model=CXRReportGenModel.from_json(segmed_cxrreportgen_json[\"model\"]),\n",
    "        dataset=segmed_dataset,\n",
    "        results=[ModelOutput.from_json(o) for o in segmed_cxrreportgen_json[\"results\"]],\n",
    "    )\n",
    "\n",
    "segmed_gpt4o_model_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT Samples - Pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtsamples_gpt4o_path = (\n",
    "    \"../../data/medbench/arena/mtsamples-pharmacy/sampled_mtsamples_gpt4/gpt-4o.json\"\n",
    ")\n",
    "mtsamples_dataset_path = (\n",
    "    \"../../data/medbench/arena/mtsamples-pharmacy/sampled_mtsamples_gpt4/data.json\"\n",
    ")\n",
    "\n",
    "with open(mtsamples_dataset_path, \"r\") as f:\n",
    "    mtsamples_dataset_json = json.load(f)\n",
    "    mtsamples_dataset_json[\"description\"] = (\n",
    "        \"The dataset is a collection of Clinical Notes from MTSamples.com, and the expected output is a summary targeting the pharmacy professionals.\"\n",
    "    )\n",
    "    mtsamples_dataset = Dataset.from_json(mtsamples_dataset_json)\n",
    "\n",
    "with open(mtsamples_gpt4o_path, \"r\") as f:\n",
    "    mtsamples_gpt4o_json = json.load(f)\n",
    "    mtsamples_gpt4o_model_run = ModelRun(\n",
    "        id=\"gpt-4o-sampled_mtsamples_gpt4\",\n",
    "        model=OpenAIChatModel.from_json(mtsamples_gpt4o_json[\"model\"]),\n",
    "        dataset=mtsamples_dataset,\n",
    "        results=[ModelOutput.from_json(o) for o in mtsamples_gpt4o_json[\"results\"]],\n",
    "    )\n",
    "\n",
    "mtsamples_gpt4o_model_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT Samples - Patient summary\n",
    "\n",
    "Data used in Arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/medbench/arena/mtsamples-pharmacy/formatted_multi-output-dataset.jsonl\", \"r\") as f:\n",
    "    mtsamples_summary_data = []\n",
    "    for line in f:\n",
    "        mtsamples_summary_data.append(json.loads(line))\n",
    "\n",
    "len(mtsamples_summary_data), mtsamples_summary_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs = load_arena_data(\n",
    "    dataset_name=\"mtsamples-summary\",\n",
    "    dataset_description=(\n",
    "        \"The dataset is a collection of Clinical Notes from MTSamples.com, and the objective \"\n",
    "        \"of the dataset is summarization of clinical notes into a structured patient summary.\"\n",
    "    ),\n",
    "    data_split=\"eval\",\n",
    "    data=mtsamples_summary_data,\n",
    "    data_key=\"clinical_note\",\n",
    "    output_keys=[\"gpt4o\", \"gpt4o-mini\", \"deepseek\"],\n",
    "    max_instances=None,\n",
    ")\n",
    "\n",
    "len(model_runs[\"deepseek\"].dataset.instances), model_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_text_evaluator = OpenAIReasoningModel(\n",
    "    name=settings.azure_openai_deployment,\n",
    "    version=settings.azure_openai_version,\n",
    "    endpoint=settings.azure_openai_endpoint,\n",
    "    api_key=settings.azure_openai_api_key,\n",
    "    vision_enabled=False,\n",
    "    # Prompts are defined by the evaluator runner.\n",
    "    system_prompt=\"\",\n",
    "    # Values from AI foundry playground\n",
    "    max_tokens=4000,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "llm_text_evaluator.name, llm_text_evaluator.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_vision_evaluator = OpenAIReasoningModel(\n",
    "    name=\"azure_openai_o1_deployment\",\n",
    "    version=\"azure_openai_o1_version\",\n",
    "    endpoint=\"azure_openai_o1_endpoint\",\n",
    "    api_key=\"azure_openai_o1_api_key\",\n",
    "    # Prompts are defined by the evaluator runner.\n",
    "    system_prompt=\"\",\n",
    "    vision_enabled=True,\n",
    "    # Values from AI foundry playground\n",
    "    max_tokens=4000,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "llm_vision_evaluator.name, llm_vision_evaluator.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmed_evaluators = {}\n",
    "\n",
    "evaluator = MultimodalEvaluatorRunner(\n",
    "    predictions_model_run=segmed_gpt4o_model_run, evaluator=llm_vision_evaluator\n",
    ")\n",
    "segmed_evaluators[\"gpt-4o\"] = evaluator\n",
    "\n",
    "evaluator = MultimodalEvaluatorRunner(\n",
    "    predictions_model_run=segmed_cxrreportgen_model_run, evaluator=llm_vision_evaluator\n",
    ")\n",
    "segmed_evaluators[\"cxrreportgen\"] = evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evaluator in segmed_evaluators.values():\n",
    "    evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_key, evaluator in segmed_evaluators.items():\n",
    "    with open(f\"../data/arena/segmed/eval/{eval_key}.json\", \"w+\") as f:\n",
    "        json.dump(evaluator.evaluator_runner._model_run.to_json(), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(segmed_cxrreportgen_model_run.dataset.instances[0].input.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = segmed_evaluators[\"cxrreportgen\"]\n",
    "e.evaluator_runner.build_user_input(e.evaluator_runner._model_run.dataset.instances[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.evaluator_runner._model_run.dataset.instances[0].input.content[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arena data eval - MTSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with simple prompt base model as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arena_evaluators = {}\n",
    "\n",
    "for model_key in model_runs:\n",
    "    evaluator = MultimodalEvaluatorRunner(\n",
    "        predictions_model_run=model_runs[model_key],\n",
    "        evaluator=llm_text_evaluator\n",
    "    )\n",
    "    arena_evaluators[model_key] = evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evaluator in arena_evaluators.values():\n",
    "    evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_key, evaluator in arena_evaluators.items():\n",
    "    with open(f\"../data/eval/{eval_key}.json\", \"w+\") as f:\n",
    "        json.dump(evaluator.evaluator_runner._model_run.to_json(), f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate using the Summary Evaluator approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arena_summ_evaluators = {}\n",
    "\n",
    "questions_generator_runner = None\n",
    "for model_key in model_runs:\n",
    "    kwargs = {}\n",
    "    if questions_generator_runner is not None:\n",
    "        kwargs[\"questions_generator_runner\"] = questions_generator_runner\n",
    "\n",
    "    evaluator = SummaryEvaluatorRunner(\n",
    "        predictions_model_run=model_runs[model_key],\n",
    "        evaluator=llm_text_evaluator,\n",
    "        skip_errors=True,\n",
    "        **kwargs,\n",
    "    )\n",
    "    arena_summ_evaluators[model_key] = evaluator\n",
    "\n",
    "    evaluator.evaluate()\n",
    "\n",
    "    if questions_generator_runner is None:\n",
    "        questions_generator_runner = evaluator.questions_generator_runner\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_key in model_runs:\n",
    "    print(model_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_key, evaluator in arena_summ_evaluators.items():\n",
    "    with open(f\"../data/eval/{eval_key}-questions.json\", \"w+\") as f:\n",
    "        json.dump(evaluator.questions_generator_runner._model_run.to_json(), f, indent=2)\n",
    "    with open(f\"../data/eval/{eval_key}-answers.json\", \"w+\") as f:\n",
    "        json.dump(evaluator.answerer_runner._model_run.to_json(), f, indent=2)\n",
    "    with open(f\"../data/eval/{eval_key}-summ.json\", \"w+\") as f:\n",
    "        json.dump(evaluator.evaluator_runner._model_run.to_json(), f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MT Samples - Pharmacy eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultimodalEvaluatorRunner(\n",
    "    predictions_model_run=mtsamples_gpt4o_model_run,\n",
    "    evaluator=llm_text_evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write this to a file: evaluator._model_run.to_json()\n",
    "with open(\"../../data/medbench/arena/mtsamples-pharmacy/sampled_mtsamples_gpt4/gpt-4o-evaluated.json\", \"w+\") as f:\n",
    "    f.write(json.dumps(evaluator.predictions_model_run.to_json()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medbench-azf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
