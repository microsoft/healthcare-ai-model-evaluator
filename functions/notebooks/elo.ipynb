{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELO calculation\n",
    "\n",
    "Before going into the formulas, a few notes:\n",
    "- The change in ELO ratings depends mainly on the K-factor (development coefficient). A higher K results in higher variance in the rating, while a lower K results in lower variance.\n",
    "- ELO ratings also take into account the difference in ratings between players to calculate the probability of winning or probability of scoring.\n",
    "- ELO rating changes are determined over a \"rating period\" (e.g.: event or tournament)\n",
    "\n",
    "ELO has two formulas:\n",
    "- Probability of winning: `1/(1+10^(-D/400))`, where `D` is the rating difference.\n",
    "- Rating change formula: `K * (score - win_probability)`, where K is the development coefficient, and the score is 1 for a win, 0.5 for a draw and 0 for a loss.\n",
    "    - For multiple games within a rating period: `K * Sum(score - win_probability)`\n",
    "\n",
    "The K used is thus the main factor for changing ELO ratings. FIDE handbook recommends:\n",
    "> K = 40 for a player new to the rating list until he has completed events with at least 30 games.\n",
    ">\n",
    "> K = 20 as long as a player's rating remains under 2400.\n",
    ">\n",
    "> K = 10 once a player's published rating has reached 2400 and remains at that level subsequently, even if the rating drops below 2400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winning_prob(self_rating, opponent_rating):\n",
    "    \"\"\"\n",
    "    Calculate the winning probability using the Elo rating system.\n",
    "\n",
    "    Args:\n",
    "        self_rating (float): The rating of the player whose winning probability we want to calculate.\n",
    "        opponent_rating (float): The rating of the opponent.\n",
    "\n",
    "    Returns:\n",
    "        float: The probability of winning.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + 10 ** ((opponent_rating - self_rating) / 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_rating(self_rating: int, opponent_rating: int, score: float, k: int = 30):\n",
    "    \"\"\"\n",
    "    Calculate the new rating after a match using the Elo rating system.\n",
    "\n",
    "    Args:\n",
    "        self_rating (int): The rating of the player whose new rating we want to calculate.\n",
    "        opponent_rating (int): The rating of the opponent.\n",
    "        score (float): The score of the player (1 for win, 0.5 for draw, 0 for loss).\n",
    "\n",
    "    Returns:\n",
    "        int: The new rating.\n",
    "    \"\"\"\n",
    "    expected_score = winning_prob(self_rating, opponent_rating)\n",
    "    return round(self_rating + k * (score - expected_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rating_change(starting_rating: int, opponent_ratings: list[int], scores: list[float], k: int = 30):\n",
    "    \"\"\"\n",
    "    Calculate the new rating after multiple matches.\n",
    "\n",
    "    Args:\n",
    "        starting_rating (int): The initial rating of the player.\n",
    "        opponent_ratings (list[int]): A list of opponent ratings.\n",
    "        scores (list[float]): A list of scores corresponding to each match.\n",
    "        k (int): The K-factor used in the Elo rating system.\n",
    "\n",
    "    Returns:\n",
    "        int: The new rating after all matches.\n",
    "    \"\"\"\n",
    "    winning_probabilities = [winning_prob(starting_rating, opponent_rating) for opponent_rating in opponent_ratings]\n",
    "\n",
    "    rating_change = 0\n",
    "    for score, winning_probability in zip(scores, winning_probabilities):\n",
    "        rating_change += score - winning_probability\n",
    "    \n",
    "    return round(k * rating_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick analysis of rating period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_rating = 1656\n",
    "opponent_ratings = [1763, 1700, 1800]\n",
    "scores = [1, 0.5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rating after match against 1763 with score 1: 1675\n",
      "New rating after match against 1700 with score 0.5: 1676\n",
      "New rating after match against 1800 with score 1: 1696\n",
      "Final rating after all matches: 1696 (40 change)\n"
     ]
    }
   ],
   "source": [
    "new_rating = starting_rating\n",
    "for opponent_rating, score in zip(opponent_ratings, scores):\n",
    "    new_rating = calculate_new_rating(new_rating, opponent_rating, score)\n",
    "    print(f\"New rating after match against {opponent_rating} with score {score}: {new_rating}\")\n",
    "\n",
    "print(f\"Final rating after all matches: {new_rating} ({new_rating - starting_rating} change)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating change after all matches: 42\n",
      "New rating after all matches: 1698\n"
     ]
    }
   ],
   "source": [
    "rating_change = calculate_rating_change(starting_rating, opponent_ratings, scores, k=30)\n",
    "\n",
    "print(f\"Rating change after all matches: {rating_change}\")\n",
    "print(f\"New rating after all matches: {starting_rating + rating_change}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELO scores from pre-calculated metrics\n",
    "\n",
    "For simplicity, we will use a rating period of 1 game, and a fixed K of 30\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNC_OUTPUT_PATH = \"../data/metrics/medhelm-mixed/func_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>ehr_sql</td>\n",
       "      <td>id9852</td>\n",
       "      <td>ehr_sql_execution_accuracy</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>ehr_sql</td>\n",
       "      <td>id9852</td>\n",
       "      <td>ehr_sql_query_validity</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>ehr_sql</td>\n",
       "      <td>id9852</td>\n",
       "      <td>ehr_sql_precision_answerable</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>ehr_sql</td>\n",
       "      <td>id9852</td>\n",
       "      <td>ehr_sql_recall_answerable</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>ehr_sql</td>\n",
       "      <td>id9852</td>\n",
       "      <td>ehr_sql_total_predicted_answerable</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378363</th>\n",
       "      <td>google/gemini-1.5-pro-001</td>\n",
       "      <td>pubmed_qa</td>\n",
       "      <td>id999</td>\n",
       "      <td>quasi_prefix_exact_match</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378364</th>\n",
       "      <td>google/gemini-1.5-pro-001</td>\n",
       "      <td>pubmed_qa</td>\n",
       "      <td>id999</td>\n",
       "      <td>quasi_prefix_exact_match@5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378365</th>\n",
       "      <td>google/gemini-1.5-pro-001</td>\n",
       "      <td>pubmed_qa</td>\n",
       "      <td>id999</td>\n",
       "      <td>logprob</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378366</th>\n",
       "      <td>google/gemini-1.5-pro-001</td>\n",
       "      <td>pubmed_qa</td>\n",
       "      <td>id999</td>\n",
       "      <td>num_perplexity_tokens</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378367</th>\n",
       "      <td>google/gemini-1.5-pro-001</td>\n",
       "      <td>pubmed_qa</td>\n",
       "      <td>id999</td>\n",
       "      <td>num_bytes</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2378368 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model_name dataset_name instance_id  \\\n",
       "0        microsoft/phi-3.5-mini-instruct      ehr_sql      id9852   \n",
       "1        microsoft/phi-3.5-mini-instruct      ehr_sql      id9852   \n",
       "2        microsoft/phi-3.5-mini-instruct      ehr_sql      id9852   \n",
       "3        microsoft/phi-3.5-mini-instruct      ehr_sql      id9852   \n",
       "4        microsoft/phi-3.5-mini-instruct      ehr_sql      id9852   \n",
       "...                                  ...          ...         ...   \n",
       "2378363        google/gemini-1.5-pro-001    pubmed_qa       id999   \n",
       "2378364        google/gemini-1.5-pro-001    pubmed_qa       id999   \n",
       "2378365        google/gemini-1.5-pro-001    pubmed_qa       id999   \n",
       "2378366        google/gemini-1.5-pro-001    pubmed_qa       id999   \n",
       "2378367        google/gemini-1.5-pro-001    pubmed_qa       id999   \n",
       "\n",
       "                                metric_name  metric_value  \n",
       "0                ehr_sql_execution_accuracy           0.0  \n",
       "1                    ehr_sql_query_validity           0.0  \n",
       "2              ehr_sql_precision_answerable           1.0  \n",
       "3                 ehr_sql_recall_answerable           1.0  \n",
       "4        ehr_sql_total_predicted_answerable           1.0  \n",
       "...                                     ...           ...  \n",
       "2378363            quasi_prefix_exact_match           0.0  \n",
       "2378364          quasi_prefix_exact_match@5           0.0  \n",
       "2378365                             logprob           0.0  \n",
       "2378366               num_perplexity_tokens           0.0  \n",
       "2378367                           num_bytes           0.0  \n",
       "\n",
       "[2378368 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = []\n",
    "for file in os.listdir(FUNC_OUTPUT_PATH):\n",
    "    if not file.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    with open(os.path.join(FUNC_OUTPUT_PATH, file), 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    model_name = data[\"original_run\"][\"model_run\"][\"model\"][\"name\"]\n",
    "    dataset_name = data[\"original_run\"][\"model_run\"][\"dataset\"][\"name\"]\n",
    "\n",
    "    for instance_metrics in data[\"metrics_results\"][\"instance_level_metrics\"]:\n",
    "        instance_id = instance_metrics[\"instance_id\"]\n",
    "        for metric_name, metric_value in instance_metrics.items():\n",
    "            if metric_name == \"instance_id\":\n",
    "                continue\n",
    "            metrics.append({\n",
    "                \"model_name\": model_name,\n",
    "                \"dataset_name\": dataset_name,\n",
    "                \"instance_id\": instance_id,\n",
    "                \"metric_name\": metric_name,\n",
    "                \"metric_value\": metric_value,\n",
    "            })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95272</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>rouge_1</td>\n",
       "      <td>0.491192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95273</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>rouge_2</td>\n",
       "      <td>0.211838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95274</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>rouge_l</td>\n",
       "      <td>0.296373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95275</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>summarization_coverage</td>\n",
       "      <td>0.686893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95276</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>summarization_density</td>\n",
       "      <td>1.774272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291845</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>prompt_truncated</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291846</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>max_prob</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291847</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>logprob</td>\n",
       "      <td>-0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291848</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>num_perplexity_tokens</td>\n",
       "      <td>512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291849</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>num_bytes</td>\n",
       "      <td>1846.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215694 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model_name   dataset_name instance_id  \\\n",
       "95272                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "95273                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "95274                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "95275                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "95276                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "...                                  ...            ...         ...   \n",
       "2291845  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "2291846  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "2291847  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "2291848  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "2291849  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "\n",
       "                    metric_name  metric_value  \n",
       "95272                   rouge_1      0.491192  \n",
       "95273                   rouge_2      0.211838  \n",
       "95274                   rouge_l      0.296373  \n",
       "95275    summarization_coverage      0.686893  \n",
       "95276     summarization_density      1.774272  \n",
       "...                         ...           ...  \n",
       "2291845        prompt_truncated      0.000000  \n",
       "2291846                max_prob      0.500000  \n",
       "2291847                 logprob     -0.693147  \n",
       "2291848   num_perplexity_tokens    512.000000  \n",
       "2291849               num_bytes   1846.000000  \n",
       "\n",
       "[215694 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out multiple choice datasets\n",
    "datasets_with_bert_score = metrics_df[metrics_df[\"metric_name\"] == \"bert_score\"][\"dataset_name\"].unique()\n",
    "\n",
    "metrics_df = metrics_df[metrics_df[\"dataset_name\"].isin(datasets_with_bert_score)]\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95272</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>rouge_1</td>\n",
       "      <td>0.491192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95273</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>rouge_2</td>\n",
       "      <td>0.211838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95274</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>rouge_l</td>\n",
       "      <td>0.296373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95275</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>summarization_coverage</td>\n",
       "      <td>0.686893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95276</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id67</td>\n",
       "      <td>summarization_density</td>\n",
       "      <td>1.774272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280132</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>BERTScore-F</td>\n",
       "      <td>0.737665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280133</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>0.737665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280134</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>rouge1</td>\n",
       "      <td>0.282486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280135</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>rouge2</td>\n",
       "      <td>0.034026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280136</th>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>medication_qa</td>\n",
       "      <td>id688</td>\n",
       "      <td>rougeL</td>\n",
       "      <td>0.131827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97412 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              model_name   dataset_name instance_id  \\\n",
       "95272                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "95273                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "95274                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "95275                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "95276                 openai/gpt-4o-mini      aci_bench        id67   \n",
       "...                                  ...            ...         ...   \n",
       "2280132  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "2280133  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "2280134  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "2280135  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "2280136  microsoft/phi-3.5-mini-instruct  medication_qa       id688   \n",
       "\n",
       "                    metric_name  metric_value  \n",
       "95272                   rouge_1      0.491192  \n",
       "95273                   rouge_2      0.211838  \n",
       "95274                   rouge_l      0.296373  \n",
       "95275    summarization_coverage      0.686893  \n",
       "95276     summarization_density      1.774272  \n",
       "...                         ...           ...  \n",
       "2280132             BERTScore-F      0.737665  \n",
       "2280133              bert_score      0.737665  \n",
       "2280134                  rouge1      0.282486  \n",
       "2280135                  rouge2      0.034026  \n",
       "2280136                  rougeL      0.131827  \n",
       "\n",
       "[97412 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out unwanted metrics\n",
    "unwanted_metrics = [\n",
    "    \"num_references\",\n",
    "    \"num_train_trials\",\n",
    "    \"num_prompt_tokens\",\n",
    "    \"num_completion_tokens\",\n",
    "    \"num_output_tokens\",\n",
    "    \"inference_runtime\",\n",
    "    \"batch_size\",\n",
    "    \"finish_reason_length\",\n",
    "    \"finish_reason_stop\",\n",
    "    \"finish_reason_endoftext\",\n",
    "    \"finish_reason_unknown\",\n",
    "    \"num_train_instances\",\n",
    "    \"prompt_truncated\",\n",
    "    \"max_prob\",\n",
    "    \"logprob\",\n",
    "    \"num_perplexity_tokens\",\n",
    "    \"num_bytes\",\n",
    "]\n",
    "\n",
    "metrics_df = metrics_df[~metrics_df[\"metric_name\"].isin(unwanted_metrics)]\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1384740</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id115</td>\n",
       "      <td>rouge_1</td>\n",
       "      <td>0.476543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384741</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id115</td>\n",
       "      <td>rouge_2</td>\n",
       "      <td>0.193069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384742</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id115</td>\n",
       "      <td>rouge_l</td>\n",
       "      <td>0.204938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384743</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id115</td>\n",
       "      <td>summarization_coverage</td>\n",
       "      <td>0.595491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384744</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id115</td>\n",
       "      <td>summarization_density</td>\n",
       "      <td>1.173740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385015</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id143</td>\n",
       "      <td>BERTScore-F</td>\n",
       "      <td>0.814235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385016</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id143</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>0.814235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385017</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id143</td>\n",
       "      <td>rouge1</td>\n",
       "      <td>0.485772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385018</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id143</td>\n",
       "      <td>rouge2</td>\n",
       "      <td>0.154786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385019</th>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id143</td>\n",
       "      <td>rougeL</td>\n",
       "      <td>0.215447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model_name dataset_name instance_id  \\\n",
       "1384740  deepseek-ai/deepseek-r1    aci_bench       id115   \n",
       "1384741  deepseek-ai/deepseek-r1    aci_bench       id115   \n",
       "1384742  deepseek-ai/deepseek-r1    aci_bench       id115   \n",
       "1384743  deepseek-ai/deepseek-r1    aci_bench       id115   \n",
       "1384744  deepseek-ai/deepseek-r1    aci_bench       id115   \n",
       "...                          ...          ...         ...   \n",
       "1385015  deepseek-ai/deepseek-r1    aci_bench       id143   \n",
       "1385016  deepseek-ai/deepseek-r1    aci_bench       id143   \n",
       "1385017  deepseek-ai/deepseek-r1    aci_bench       id143   \n",
       "1385018  deepseek-ai/deepseek-r1    aci_bench       id143   \n",
       "1385019  deepseek-ai/deepseek-r1    aci_bench       id143   \n",
       "\n",
       "                    metric_name  metric_value  \n",
       "1384740                 rouge_1      0.476543  \n",
       "1384741                 rouge_2      0.193069  \n",
       "1384742                 rouge_l      0.204938  \n",
       "1384743  summarization_coverage      0.595491  \n",
       "1384744   summarization_density      1.173740  \n",
       "...                         ...           ...  \n",
       "1385015             BERTScore-F      0.814235  \n",
       "1385016              bert_score      0.814235  \n",
       "1385017                  rouge1      0.485772  \n",
       "1385018                  rouge2      0.154786  \n",
       "1385019                  rougeL      0.215447  \n",
       "\n",
       "[280 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[metrics_df[\"model_name\"] == \"deepseek-ai/deepseek-r1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aci_bench', 'mtsamples', 'medication_qa', 'mtsamples_replicate'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[\"dataset_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bert_score</th>\n",
       "      <td>6958.0</td>\n",
       "      <td>0.743877</td>\n",
       "      <td>0.058322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.709928</td>\n",
       "      <td>0.744421</td>\n",
       "      <td>0.783495</td>\n",
       "      <td>0.895537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_1</th>\n",
       "      <td>6958.0</td>\n",
       "      <td>0.226268</td>\n",
       "      <td>0.155291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.201550</td>\n",
       "      <td>0.305602</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_2</th>\n",
       "      <td>6958.0</td>\n",
       "      <td>0.061037</td>\n",
       "      <td>0.072013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012739</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>0.077245</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_l</th>\n",
       "      <td>6958.0</td>\n",
       "      <td>0.131555</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std  min       25%       50%       75%  \\\n",
       "metric_name                                                                  \n",
       "bert_score   6958.0  0.743877  0.058322  0.0  0.709928  0.744421  0.783495   \n",
       "rouge_1      6958.0  0.226268  0.155291  0.0  0.108108  0.201550  0.305602   \n",
       "rouge_2      6958.0  0.061037  0.072013  0.0  0.012739  0.035503  0.077245   \n",
       "rouge_l      6958.0  0.131555  0.086420  0.0  0.072727  0.117647  0.166667   \n",
       "\n",
       "                  max  \n",
       "metric_name            \n",
       "bert_score   0.895537  \n",
       "rouge_1      0.777778  \n",
       "rouge_2      0.714286  \n",
       "rouge_l      0.777778  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_metrics = [\n",
    "    \"rouge_1\",\n",
    "    \"rouge_2\",\n",
    "    \"rouge_l\",\n",
    "    \"bert_score\",\n",
    "]\n",
    "\n",
    "filtered_metrics_df = metrics_df[metrics_df[\"metric_name\"].isin(target_metrics)]\n",
    "\n",
    "filtered_metrics_df.groupby(\"metric_name\")[\"metric_value\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create matches for ELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_score(match: dict, draw_threshold: float = 0.03):\n",
    "    \"\"\"Calculate the match score for a given match.\n",
    "\n",
    "    Draw threshold is set to 3% by default, as this is below\n",
    "    the stdandard deviation from all matches calculated below.\n",
    "\n",
    "    Args:\n",
    "        match (dict): A dictionary containing the match results.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the match score.\n",
    "    \"\"\"\n",
    "    model_a, model_b = list(match.keys())\n",
    "    score_a = match[model_a]\n",
    "    score_b = match[model_b]\n",
    "\n",
    "    # Allow for a draw as long as results are at max 10% apart\n",
    "    if abs((score_a / score_b) - 1) < draw_threshold:\n",
    "        return {model_a: 0.5, model_b: 0.5}\n",
    "    elif score_a > score_b:\n",
    "        return {model_a: 1, model_b: 0}\n",
    "    else:\n",
    "        return {model_a: 0, model_b: 1}\n",
    "\n",
    "\n",
    "def create_matches(\n",
    "    metrics_df,\n",
    "    target_metric: str,\n",
    "    model_name_column: str = \"model_name\",\n",
    "    dataset_name_column: str = \"dataset_name\",\n",
    "    instance_id_column: str = \"instance_id\",\n",
    "    metric_name_column: str = \"metric_name\",\n",
    "    metric_value_column: str = \"metric_value\",\n",
    "):\n",
    "    \"\"\"Create matches for the Elo rating system.\n",
    "\n",
    "    Each match is a pair of two models along with the score of the match.\n",
    "\n",
    "    Returns:\n",
    "        list of dict: A list of dictionaries containing the matches.\n",
    "        Each dictionary contains the match results, the dataset name and the instance id.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    filtered_metrics_df = metrics_df[metrics_df[metric_name_column] == target_metric]\n",
    "    filtered_metrics_df = filtered_metrics_df.dropna()\n",
    "    for _, group in filtered_metrics_df.groupby(\n",
    "        [dataset_name_column, instance_id_column]\n",
    "    ):\n",
    "        # Generate matches for all combinations of models\n",
    "        models = group[model_name_column].unique()\n",
    "        for i in range(len(models)):\n",
    "            for j in range(i + 1, len(models)):\n",
    "                model_a = models[i]\n",
    "                model_b = models[j]\n",
    "\n",
    "                # Create a match dictionary\n",
    "                model_a_metric = group[group[model_name_column] == model_a][\n",
    "                    metric_value_column\n",
    "                ].values[0]\n",
    "                model_b_metric = group[group[model_name_column] == model_b][\n",
    "                    metric_value_column\n",
    "                ].values[0]\n",
    "                match_results = calculate_match_score(\n",
    "                    match={\n",
    "                        model_a: model_a_metric,\n",
    "                        model_b: model_b_metric,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                matches.append(\n",
    "                    {\n",
    "                        \"model_a\": model_a,\n",
    "                        \"model_b\": model_b,\n",
    "                        \"model_a_score\": match_results[model_a],\n",
    "                        \"model_b_score\": match_results[model_b],\n",
    "                        \"model_a_metric\": model_a_metric,\n",
    "                        \"model_b_metric\": model_b_metric,\n",
    "                        \"metric_name\": target_metric,\n",
    "                        \"dataset_name\": group[dataset_name_column].values[0],\n",
    "                        \"instance_id\": group[instance_id_column].values[0],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>model_a_score</th>\n",
       "      <th>model_b_score</th>\n",
       "      <th>model_a_metric</th>\n",
       "      <th>model_b_metric</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>instance_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>google/gemini-1.5-pro-001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.832080</td>\n",
       "      <td>0.827930</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>qwen/qwen2.5-7b-instruct</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.832080</td>\n",
       "      <td>0.841437</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>openai/gpt-4o</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.832080</td>\n",
       "      <td>0.846204</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>microsoft/phi-3.5-mini-instruct</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.832080</td>\n",
       "      <td>0.825319</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>deepseek-ai/deepseek-r1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.832080</td>\n",
       "      <td>0.785560</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>aci_bench</td>\n",
       "      <td>id100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14350</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>meta/llama-3.3-70b-instruct</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.764106</td>\n",
       "      <td>0.760645</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>mtsamples_replicate</td>\n",
       "      <td>id9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14351</th>\n",
       "      <td>openai/gpt-4o-mini</td>\n",
       "      <td>openai/gpt-4o</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.764106</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>mtsamples_replicate</td>\n",
       "      <td>id9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14352</th>\n",
       "      <td>google/gemini-1.5-pro-001</td>\n",
       "      <td>meta/llama-3.3-70b-instruct</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.736867</td>\n",
       "      <td>0.760645</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>mtsamples_replicate</td>\n",
       "      <td>id9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14353</th>\n",
       "      <td>google/gemini-1.5-pro-001</td>\n",
       "      <td>openai/gpt-4o</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.736867</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>mtsamples_replicate</td>\n",
       "      <td>id9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14354</th>\n",
       "      <td>meta/llama-3.3-70b-instruct</td>\n",
       "      <td>openai/gpt-4o</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.760645</td>\n",
       "      <td>0.766231</td>\n",
       "      <td>bert_score</td>\n",
       "      <td>mtsamples_replicate</td>\n",
       "      <td>id9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14355 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model_a                          model_b  \\\n",
       "0               openai/gpt-4o-mini        google/gemini-1.5-pro-001   \n",
       "1               openai/gpt-4o-mini         qwen/qwen2.5-7b-instruct   \n",
       "2               openai/gpt-4o-mini                    openai/gpt-4o   \n",
       "3               openai/gpt-4o-mini  microsoft/phi-3.5-mini-instruct   \n",
       "4               openai/gpt-4o-mini          deepseek-ai/deepseek-r1   \n",
       "...                            ...                              ...   \n",
       "14350           openai/gpt-4o-mini      meta/llama-3.3-70b-instruct   \n",
       "14351           openai/gpt-4o-mini                    openai/gpt-4o   \n",
       "14352    google/gemini-1.5-pro-001      meta/llama-3.3-70b-instruct   \n",
       "14353    google/gemini-1.5-pro-001                    openai/gpt-4o   \n",
       "14354  meta/llama-3.3-70b-instruct                    openai/gpt-4o   \n",
       "\n",
       "       model_a_score  model_b_score  model_a_metric  model_b_metric  \\\n",
       "0                0.5            0.5        0.832080        0.827930   \n",
       "1                0.5            0.5        0.832080        0.841437   \n",
       "2                0.5            0.5        0.832080        0.846204   \n",
       "3                0.5            0.5        0.832080        0.825319   \n",
       "4                1.0            0.0        0.832080        0.785560   \n",
       "...              ...            ...             ...             ...   \n",
       "14350            0.5            0.5        0.764106        0.760645   \n",
       "14351            0.5            0.5        0.764106        0.766231   \n",
       "14352            0.0            1.0        0.736867        0.760645   \n",
       "14353            0.0            1.0        0.736867        0.766231   \n",
       "14354            0.5            0.5        0.760645        0.766231   \n",
       "\n",
       "      metric_name         dataset_name instance_id  \n",
       "0      bert_score            aci_bench       id100  \n",
       "1      bert_score            aci_bench       id100  \n",
       "2      bert_score            aci_bench       id100  \n",
       "3      bert_score            aci_bench       id100  \n",
       "4      bert_score            aci_bench       id100  \n",
       "...           ...                  ...         ...  \n",
       "14350  bert_score  mtsamples_replicate         id9  \n",
       "14351  bert_score  mtsamples_replicate         id9  \n",
       "14352  bert_score  mtsamples_replicate         id9  \n",
       "14353  bert_score  mtsamples_replicate         id9  \n",
       "14354  bert_score  mtsamples_replicate         id9  \n",
       "\n",
       "[14355 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = create_matches(\n",
    "    metrics_df,\n",
    "    target_metric=\"bert_score\",\n",
    "    model_name_column=\"model_name\",\n",
    "    dataset_name_column=\"dataset_name\",\n",
    "    instance_id_column=\"instance_id\",\n",
    "    metric_name_column=\"metric_name\",\n",
    "    metric_value_column=\"metric_value\",\n",
    ")\n",
    "\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check standard deviation in 1on1 games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    14355.000000\n",
       "mean         1.011275\n",
       "std          0.052329\n",
       "min          0.589143\n",
       "25%          0.983533\n",
       "50%          1.007395\n",
       "75%          1.035708\n",
       "max          1.688747\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_0_matches_df = matches_df[\n",
    "    (matches_df[\"model_a_metric\"] != 0) & (matches_df[\"model_b_metric\"] != 0)\n",
    "]\n",
    "\n",
    "(non_0_matches_df[\"model_a_metric\"] / non_0_matches_df[\"model_b_metric\"]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check general results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_a_score\n",
       "0.5    7983\n",
       "1.0    4176\n",
       "0.0    2196\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_df[\"model_a_score\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate scores\n",
    "\n",
    "Aggregated across all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'microsoft/phi-3.5-mini-instruct': 1416,\n",
       " 'google/gemini-1.5-pro-001': 1470,\n",
       " 'openai/gpt-4o-mini': 1625,\n",
       " 'qwen/qwen2.5-7b-instruct': 1489,\n",
       " 'meta/llama-3.3-70b-instruct': 1612,\n",
       " 'openai/gpt-4o': 1547,\n",
       " 'deepseek-ai/deepseek-r1': 1341}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the matches\n",
    "shuffled_matches_df = matches_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "elo_ratings = {}\n",
    "for row in shuffled_matches_df.iterrows():\n",
    "    model_a = row[1][\"model_a\"]\n",
    "    model_b = row[1][\"model_b\"]\n",
    "    score_a = row[1][\"model_a_score\"]\n",
    "    score_b = row[1][\"model_b_score\"]\n",
    "    \n",
    "    if model_a not in elo_ratings:\n",
    "        elo_ratings[model_a] = 1500\n",
    "    if model_b not in elo_ratings:\n",
    "        elo_ratings[model_b] = 1500\n",
    "    model_a_rating = elo_ratings[model_a]\n",
    "    model_b_rating = elo_ratings[model_b]\n",
    "\n",
    "    # NOTE: Rating changes are usually proportional (the same for both models)\n",
    "    # but it would be different if we use different K-factors for each model.\n",
    "    model_a_rating_change = calculate_rating_change(\n",
    "        starting_rating=model_a_rating,\n",
    "        opponent_ratings=[model_b_rating],\n",
    "        scores=[score_a],\n",
    "        k=30\n",
    "    )\n",
    "    model_b_rating_change = calculate_rating_change(\n",
    "        starting_rating=model_b_rating,\n",
    "        opponent_ratings=[model_a_rating],\n",
    "        scores=[score_b],\n",
    "        k=30\n",
    "    )\n",
    "\n",
    "    elo_ratings[model_a] += model_a_rating_change\n",
    "    elo_ratings[model_b] += model_b_rating_change\n",
    "\n",
    "elo_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"microsoft/phi-3.5-mini-instruct\": {\n",
      "        \"bert_score_elo\": 1416\n",
      "    },\n",
      "    \"google/gemini-1.5-pro-001\": {\n",
      "        \"bert_score_elo\": 1470\n",
      "    },\n",
      "    \"openai/gpt-4o-mini\": {\n",
      "        \"bert_score_elo\": 1625\n",
      "    },\n",
      "    \"qwen/qwen2.5-7b-instruct\": {\n",
      "        \"bert_score_elo\": 1489\n",
      "    },\n",
      "    \"meta/llama-3.3-70b-instruct\": {\n",
      "        \"bert_score_elo\": 1612\n",
      "    },\n",
      "    \"openai/gpt-4o\": {\n",
      "        \"bert_score_elo\": 1547\n",
      "    },\n",
      "    \"deepseek-ai/deepseek-r1\": {\n",
      "        \"bert_score_elo\": 1341\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "metrics_dict = {}\n",
    "for model, rating in elo_ratings.items():\n",
    "    metrics_dict[model] = {\"bert_score_elo\": rating}\n",
    "\n",
    "print(json.dumps(metrics_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"aci_bench\": {\n",
      "        \"openai/gpt-4o-mini\": {\n",
      "            \"bert_score_elo\": 1550\n",
      "        },\n",
      "        \"qwen/qwen2.5-7b-instruct\": {\n",
      "            \"bert_score_elo\": 1492\n",
      "        },\n",
      "        \"openai/gpt-4o\": {\n",
      "            \"bert_score_elo\": 1574\n",
      "        },\n",
      "        \"microsoft/phi-3.5-mini-instruct\": {\n",
      "            \"bert_score_elo\": 1447\n",
      "        },\n",
      "        \"google/gemini-1.5-pro-001\": {\n",
      "            \"bert_score_elo\": 1548\n",
      "        },\n",
      "        \"meta/llama-3.3-70b-instruct\": {\n",
      "            \"bert_score_elo\": 1546\n",
      "        },\n",
      "        \"deepseek-ai/deepseek-r1\": {\n",
      "            \"bert_score_elo\": 1343\n",
      "        }\n",
      "    },\n",
      "    \"medication_qa\": {\n",
      "        \"openai/gpt-4o-mini\": {\n",
      "            \"bert_score_elo\": 1612\n",
      "        },\n",
      "        \"qwen/qwen2.5-7b-instruct\": {\n",
      "            \"bert_score_elo\": 1478\n",
      "        },\n",
      "        \"meta/llama-3.3-70b-instruct\": {\n",
      "            \"bert_score_elo\": 1596\n",
      "        },\n",
      "        \"microsoft/phi-3.5-mini-instruct\": {\n",
      "            \"bert_score_elo\": 1350\n",
      "        },\n",
      "        \"openai/gpt-4o\": {\n",
      "            \"bert_score_elo\": 1538\n",
      "        },\n",
      "        \"google/gemini-1.5-pro-001\": {\n",
      "            \"bert_score_elo\": 1426\n",
      "        }\n",
      "    },\n",
      "    \"mtsamples\": {\n",
      "        \"microsoft/phi-3.5-mini-instruct\": {\n",
      "            \"bert_score_elo\": 1587\n",
      "        },\n",
      "        \"google/gemini-1.5-pro-001\": {\n",
      "            \"bert_score_elo\": 1489\n",
      "        },\n",
      "        \"openai/gpt-4o\": {\n",
      "            \"bert_score_elo\": 1532\n",
      "        },\n",
      "        \"openai/gpt-4o-mini\": {\n",
      "            \"bert_score_elo\": 1453\n",
      "        },\n",
      "        \"meta/llama-3.3-70b-instruct\": {\n",
      "            \"bert_score_elo\": 1485\n",
      "        },\n",
      "        \"qwen/qwen2.5-7b-instruct\": {\n",
      "            \"bert_score_elo\": 1454\n",
      "        }\n",
      "    },\n",
      "    \"mtsamples_replicate\": {\n",
      "        \"microsoft/phi-3.5-mini-instruct\": {\n",
      "            \"bert_score_elo\": 1603\n",
      "        },\n",
      "        \"meta/llama-3.3-70b-instruct\": {\n",
      "            \"bert_score_elo\": 1522\n",
      "        },\n",
      "        \"openai/gpt-4o-mini\": {\n",
      "            \"bert_score_elo\": 1476\n",
      "        },\n",
      "        \"openai/gpt-4o\": {\n",
      "            \"bert_score_elo\": 1531\n",
      "        },\n",
      "        \"qwen/qwen2.5-7b-instruct\": {\n",
      "            \"bert_score_elo\": 1436\n",
      "        },\n",
      "        \"google/gemini-1.5-pro-001\": {\n",
      "            \"bert_score_elo\": 1432\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the matches\n",
    "shuffled_matches_df = matches_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Initialize a dictionary to store ELO ratings for each dataset\n",
    "elo_ratings_by_dataset = {}\n",
    "\n",
    "# Group matches by dataset_name\n",
    "for dataset_name, group in shuffled_matches_df.groupby(\"dataset_name\"):\n",
    "    elo_ratings = {}\n",
    "    for row in group.iterrows():\n",
    "        model_a = row[1][\"model_a\"]\n",
    "        model_b = row[1][\"model_b\"]\n",
    "        score_a = row[1][\"model_a_score\"]\n",
    "        score_b = row[1][\"model_b_score\"]\n",
    "\n",
    "        if model_a not in elo_ratings:\n",
    "            elo_ratings[model_a] = 1500\n",
    "        if model_b not in elo_ratings:\n",
    "            elo_ratings[model_b] = 1500\n",
    "\n",
    "        model_a_rating = elo_ratings[model_a]\n",
    "        model_b_rating = elo_ratings[model_b]\n",
    "\n",
    "        # Calculate rating changes for both models\n",
    "        model_a_rating_change = calculate_rating_change(\n",
    "            starting_rating=model_a_rating,\n",
    "            opponent_ratings=[model_b_rating],\n",
    "            scores=[score_a],\n",
    "            k=30\n",
    "        )\n",
    "        model_b_rating_change = calculate_rating_change(\n",
    "            starting_rating=model_b_rating,\n",
    "            opponent_ratings=[model_a_rating],\n",
    "            scores=[score_b],\n",
    "            k=30\n",
    "        )\n",
    "\n",
    "        elo_ratings[model_a] += model_a_rating_change\n",
    "        elo_ratings[model_b] += model_b_rating_change\n",
    "\n",
    "    # Store the ELO ratings for the current dataset\n",
    "    elo_ratings_by_dataset[dataset_name] = elo_ratings\n",
    "\n",
    "# Convert the results into a dictionary for easier inspection\n",
    "metrics_dict = {}\n",
    "for dataset_name, ratings in elo_ratings_by_dataset.items():\n",
    "    metrics_dict[dataset_name] = {model: {\"bert_score_elo\": rating} for model, rating in ratings.items()}\n",
    "\n",
    "# Print the results\n",
    "print(json.dumps(metrics_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: aci_bench\n",
      "{\n",
      "    \"openai/gpt-4o-mini\": {\n",
      "        \"bert_score_elo\": 1550\n",
      "    },\n",
      "    \"qwen/qwen2.5-7b-instruct\": {\n",
      "        \"bert_score_elo\": 1492\n",
      "    },\n",
      "    \"openai/gpt-4o\": {\n",
      "        \"bert_score_elo\": 1574\n",
      "    },\n",
      "    \"microsoft/phi-3.5-mini-instruct\": {\n",
      "        \"bert_score_elo\": 1447\n",
      "    },\n",
      "    \"google/gemini-1.5-pro-001\": {\n",
      "        \"bert_score_elo\": 1548\n",
      "    },\n",
      "    \"meta/llama-3.3-70b-instruct\": {\n",
      "        \"bert_score_elo\": 1546\n",
      "    },\n",
      "    \"deepseek-ai/deepseek-r1\": {\n",
      "        \"bert_score_elo\": 1343\n",
      "    }\n",
      "}\n",
      "Dataset: medication_qa\n",
      "{\n",
      "    \"openai/gpt-4o-mini\": {\n",
      "        \"bert_score_elo\": 1612\n",
      "    },\n",
      "    \"qwen/qwen2.5-7b-instruct\": {\n",
      "        \"bert_score_elo\": 1478\n",
      "    },\n",
      "    \"meta/llama-3.3-70b-instruct\": {\n",
      "        \"bert_score_elo\": 1596\n",
      "    },\n",
      "    \"microsoft/phi-3.5-mini-instruct\": {\n",
      "        \"bert_score_elo\": 1350\n",
      "    },\n",
      "    \"openai/gpt-4o\": {\n",
      "        \"bert_score_elo\": 1538\n",
      "    },\n",
      "    \"google/gemini-1.5-pro-001\": {\n",
      "        \"bert_score_elo\": 1426\n",
      "    }\n",
      "}\n",
      "Dataset: mtsamples\n",
      "{\n",
      "    \"microsoft/phi-3.5-mini-instruct\": {\n",
      "        \"bert_score_elo\": 1587\n",
      "    },\n",
      "    \"google/gemini-1.5-pro-001\": {\n",
      "        \"bert_score_elo\": 1489\n",
      "    },\n",
      "    \"openai/gpt-4o\": {\n",
      "        \"bert_score_elo\": 1532\n",
      "    },\n",
      "    \"openai/gpt-4o-mini\": {\n",
      "        \"bert_score_elo\": 1453\n",
      "    },\n",
      "    \"meta/llama-3.3-70b-instruct\": {\n",
      "        \"bert_score_elo\": 1485\n",
      "    },\n",
      "    \"qwen/qwen2.5-7b-instruct\": {\n",
      "        \"bert_score_elo\": 1454\n",
      "    }\n",
      "}\n",
      "Dataset: mtsamples_replicate\n",
      "{\n",
      "    \"microsoft/phi-3.5-mini-instruct\": {\n",
      "        \"bert_score_elo\": 1603\n",
      "    },\n",
      "    \"meta/llama-3.3-70b-instruct\": {\n",
      "        \"bert_score_elo\": 1522\n",
      "    },\n",
      "    \"openai/gpt-4o-mini\": {\n",
      "        \"bert_score_elo\": 1476\n",
      "    },\n",
      "    \"openai/gpt-4o\": {\n",
      "        \"bert_score_elo\": 1531\n",
      "    },\n",
      "    \"qwen/qwen2.5-7b-instruct\": {\n",
      "        \"bert_score_elo\": 1436\n",
      "    },\n",
      "    \"google/gemini-1.5-pro-001\": {\n",
      "        \"bert_score_elo\": 1432\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, ratings in elo_ratings_by_dataset.items():\n",
    "    metrics_dict = {}\n",
    "    for model, rating in ratings.items():\n",
    "        metrics_dict[model] = {\"bert_score_elo\": rating}\n",
    "\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(json.dumps(metrics_dict, indent=4))\n",
    "\n",
    "    # Save the metrics to a JSON file\n",
    "    with open(f\"elo_ratings_{dataset_name}.json\", \"w\") as f:\n",
    "        json.dump(metrics_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medbench-metrics-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
