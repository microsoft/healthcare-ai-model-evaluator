{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedHelm data collection\n",
    "\n",
    "## Downloading the data\n",
    "\n",
    "This notebook requires you to already have downloaded the data you want to transform. Follow [Helm's guide to download the raw results.](https://crfm-helm.readthedocs.io/en/stable/downloading_raw_results/).\n",
    "\n",
    "You can also manually explore the [data available in the browser](https://console.cloud.google.com/storage/browser/crfm-helm-public/medhelm?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&inv=1&invt=AbtJ_g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"../\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedHelm data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDHELM_DATA_PATH = \"../../data/medhelm/benchmark_output\"\n",
    "MEDHELM_RUN_PATH = os.path.join(MEDHELM_DATA_PATH, \"runs/v1.0.0\")\n",
    "\n",
    "OUTPUT_PATH = \"../../data/medhelm/medbench_connection/medhelm-fixed\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add below the datasets you want to collect data from.\n",
    "\n",
    "â—If you want to collect and transform the actual dataset and model outputs, only text-based datasets are supported by the transformation logic.\n",
    "\n",
    "ðŸ’¡For metrics collection, however, any dataset should work. Although the `model_run` will be incomplete, as it won't include images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DATASETS = [\n",
    "    \"aci_bench\",\n",
    "    \"ehr_sql\",\n",
    "    \"head_qa\",\n",
    "    \"medbullets\",\n",
    "    \"medcalc_bench\",\n",
    "    \"medec\",\n",
    "    \"medhallu\",\n",
    "    # \"medi_qa\",\n",
    "    \"medication_qa\",\n",
    "    # \"mimiciv_billing_code\",\n",
    "    \"mtsamples_procedures\",\n",
    "    \"mtsamples_replicate\",\n",
    "    \"pubmed_qa\",\n",
    "    \"race_based_med\",\n",
    "    # \"med_dialog\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark multiple choice datasets, as their references needs mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIPLE_CHOICE_DATASETS = [\n",
    "    \"head_qa\",\n",
    "    \"medbullet\", # There's a typo in the helm data.\n",
    "    \"pubmed_qa\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, Helm uses different names for their metrics. All of the MedHelm metrics will be kept with their original name, however we also duplicate specific ones so that Arena can display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_MAP = {\n",
    "    \"BERTScore-F\": \"bert_score\",\n",
    "    \"rouge_1\": \"rouge1\",\n",
    "    \"rouge_2\": \"rouge2\",\n",
    "    \"rouge_l\": \"rougeL\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helm splits stores all their data in the files defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_METADATA_FILENAME: str = \"run_spec.json\"\n",
    "\n",
    "# Contains `results` and `instance`\n",
    "RUN_RESULTS_FILENAME: str = \"scenario_state.json\"\n",
    "\n",
    "DATASET_METADATA_FILENAME: str = \"scenario.json\"\n",
    "DATASET_INSTANCES_FILENAME: str = \"instances.json\"\n",
    "\n",
    "DATASET_METRICS_FILENAME: str = \"stats.json\"\n",
    "INDIVIDUAL_METRICS_FILENAME: str = \"per_instance_stats.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mapping of `medbench` data classes to `medhelm`.\n",
    "\n",
    "```yaml\n",
    "ModelRun:\n",
    "    Model:\n",
    "        name: \n",
    "            - run_spec.adapter_spec.model\n",
    "            - scenario_state.adapter_spec.model\n",
    "        version:\n",
    "\n",
    "    Dataset:\n",
    "        name: scenario.name\n",
    "        description: scenario.description\n",
    "        instances:\n",
    "            - instances\n",
    "            - scenario_state.request_states[].instance\n",
    "    \n",
    "    ModelOutput[]: scenario_state.request_states[].result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and transform MedHelm data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from medbench.datasets import (\n",
    "    CORRECT_TAG,\n",
    "    Data,\n",
    "    Dataset,\n",
    "    Instance,\n",
    "    MediaObject,\n",
    "    Reference,\n",
    ")\n",
    "from medbench.models import Model, ModelOutput, ModelRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for transforming helm run and metrics into medbench format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_helm_run(helm_run_path: str) -> ModelRun:\n",
    "    # Open Helm run files\n",
    "    run_metadata: dict\n",
    "    with open(\n",
    "        os.path.join(helm_run_path, RUN_METADATA_FILENAME), \"r\"\n",
    "    ) as run_metadata_file:\n",
    "        run_metadata = json.load(run_metadata_file)\n",
    "\n",
    "    dataset_metadata: dict\n",
    "    with open(\n",
    "        os.path.join(helm_run_path, DATASET_METADATA_FILENAME), \"r\"\n",
    "    ) as dataset_metadata_file:\n",
    "        dataset_metadata = json.load(dataset_metadata_file)\n",
    "\n",
    "    run_results: dict\n",
    "    with open(\n",
    "        os.path.join(helm_run_path, RUN_RESULTS_FILENAME), \"r\"\n",
    "    ) as run_results_file:\n",
    "        run_results = json.load(run_results_file)\n",
    "\n",
    "    # Initialize medbench objects\n",
    "    model = Model(\n",
    "        name=run_metadata[\"adapter_spec\"][\"model\"],\n",
    "        version=\"\",\n",
    "    )\n",
    "\n",
    "    dataset = Dataset(\n",
    "        name=dataset_metadata[\"name\"],\n",
    "        description=dataset_metadata[\"description\"],\n",
    "        instances=[],\n",
    "    )\n",
    "\n",
    "    model_run = ModelRun(\n",
    "        id=run_metadata[\"name\"],\n",
    "        model=model,\n",
    "        dataset=dataset,\n",
    "        results=[],\n",
    "    )\n",
    "\n",
    "    # Process instances and results (from scenario_state.json)\n",
    "    for request_state in run_results[\"request_states\"]:\n",
    "        instance_data = request_state[\"instance\"]\n",
    "\n",
    "        references = []\n",
    "        for i, reference_data in enumerate(instance_data[\"references\"]):\n",
    "            metadata: dict = None\n",
    "\n",
    "            if dataset.name in MULTIPLE_CHOICE_DATASETS:\n",
    "                MULTIPLE_CHOICE_LETTERS = \"ABCDEFGH\"\n",
    "                metadata = {\"multiple_choice_letter\": MULTIPLE_CHOICE_LETTERS[i]}\n",
    "\n",
    "            data = Data.from_text(\n",
    "                data=reference_data[\"output\"][\"text\"], metadata=metadata\n",
    "            )\n",
    "\n",
    "            references.append(\n",
    "                Reference(\n",
    "                    output=data,\n",
    "                    tags=[CORRECT_TAG] if \"correct\" in reference_data[\"tags\"] else [],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        instance = Instance(\n",
    "            id=instance_data[\"id\"],\n",
    "            input=Data.from_text(data=instance_data[\"input\"][\"text\"]),\n",
    "            references=references,\n",
    "            split=instance_data[\"split\"],\n",
    "            sub_split=\"\",\n",
    "            perturbation=\"\",\n",
    "            metadata=\"\",\n",
    "        )\n",
    "        dataset.instances.append(instance)\n",
    "\n",
    "        result_data = request_state[\"result\"]\n",
    "        result = ModelOutput(\n",
    "            input_id=instance_data[\"id\"],\n",
    "            completions=Data.from_text(data=result_data[\"completions\"][0][\"text\"]),\n",
    "        )\n",
    "\n",
    "        if not result_data[\"success\"]:\n",
    "            result.finish_reason = result_data[\"error\"]\n",
    "            result.error = result_data[\"error\"]\n",
    "\n",
    "        model_run.results.append(result)\n",
    "\n",
    "    return model_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_helm_metrics(\n",
    "    stats: dict,\n",
    "    aggregation_method: str,\n",
    "    target_split: str,\n",
    "    metrics_map: dict,\n",
    ") -> dict:\n",
    "    metrics = {}\n",
    "    for stat in stats:\n",
    "        if stat[\"name\"][\"split\"] != target_split:\n",
    "            continue\n",
    "        \n",
    "        if aggregation_method not in stat:\n",
    "            continue\n",
    "        \n",
    "        metrics[stat[\"name\"][\"name\"]] = stat[aggregation_method]\n",
    "\n",
    "    for helm_metric in metrics_map:\n",
    "        if helm_metric in metrics:\n",
    "            metrics[metrics_map[helm_metric]] = metrics[helm_metric]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def process_helm_metrics(\n",
    "    helm_run_path: str,\n",
    "    aggregation_method: str = \"mean\",\n",
    "    target_split: str = \"test\",\n",
    "    metrics_map: dict = METRICS_MAP,\n",
    ") -> dict:\n",
    "    # Open Helm metrics files\n",
    "    aggregated_metrics_data: dict\n",
    "    with open(\n",
    "        os.path.join(helm_run_path, DATASET_METRICS_FILENAME), \"r\"\n",
    "    ) as metrics_file:\n",
    "        aggregated_metrics_data = json.load(metrics_file)\n",
    "\n",
    "    instance_level_metrics_data: dict\n",
    "    with open(\n",
    "        os.path.join(helm_run_path, INDIVIDUAL_METRICS_FILENAME), \"r\"\n",
    "    ) as metrics_file:\n",
    "        instance_level_metrics_data = json.load(metrics_file)\n",
    "\n",
    "    # Transform metrics\n",
    "    aggregated_metrics: dict = transform_helm_metrics(\n",
    "        aggregated_metrics_data,\n",
    "        aggregation_method,\n",
    "        target_split,\n",
    "        metrics_map,\n",
    "    )\n",
    "\n",
    "    instance_level_metrics: list[dict] = []\n",
    "    for instance in instance_level_metrics_data:\n",
    "        instance_metrics = transform_helm_metrics(\n",
    "            instance[\"stats\"],\n",
    "            aggregation_method,\n",
    "            target_split,\n",
    "            metrics_map,\n",
    "        )\n",
    "        instance_metrics[\"instance_id\"] = instance[\"instance_id\"]\n",
    "\n",
    "        instance_level_metrics.append(instance_metrics)\n",
    "\n",
    "    return {\n",
    "        \"aggregated_metrics\": aggregated_metrics,\n",
    "        \"instance_level_metrics\": instance_level_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate Healthcare AI Model Evaluator metrics function output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing medication_qa:model=qwen_qwen2.5-7b-instruct\n",
      "Processing medbullets:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing race_based_med:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing medbullets:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing head_qa:language=en,category=None,model=qwen_qwen2.5-7b-instruct\n",
      "Processing race_based_med:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing ehr_sql:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing aci_bench:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing medication_qa:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing medbullets:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing pubmed_qa:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing mtsamples_procedures:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing aci_bench:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing pubmed_qa:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing medbullets:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing medication_qa:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing medcalc_bench:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing medbullets:model=qwen_qwen2.5-7b-instruct\n",
      "Processing ehr_sql:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing pubmed_qa:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing ehr_sql:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing head_qa:language=en,category=None,model=microsoft_phi-3.5-mini-instruct\n",
      "Processing head_qa:language=en,category=None,model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing medec:model=qwen_qwen2.5-7b-instruct\n",
      "Processing medbullets:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing pubmed_qa:model=qwen_qwen2.5-7b-instruct\n",
      "Processing mtsamples_replicate:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing aci_bench:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing aci_bench:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing mtsamples_replicate:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing race_based_med:model=qwen_qwen2.5-7b-instruct\n",
      "Processing medication_qa:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing medec:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing medcalc_bench:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing medcalc_bench:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing medcalc_bench:model=qwen_qwen2.5-7b-instruct\n",
      "Processing ehr_sql:model=qwen_qwen2.5-7b-instruct\n",
      "Processing medec:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing medication_qa:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing mtsamples_procedures:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing pubmed_qa:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing mtsamples_replicate:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing mtsamples_replicate:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing mtsamples_procedures:model=qwen_qwen2.5-7b-instruct\n",
      "Processing aci_bench:model=qwen_qwen2.5-7b-instruct\n",
      "Processing aci_bench:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing mtsamples_procedures:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing medication_qa:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing medec:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing medec:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing pubmed_qa:model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing medec:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing head_qa:language=en,category=None,model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing mtsamples_replicate:model=qwen_qwen2.5-7b-instruct\n",
      "Processing ehr_sql:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing ehr_sql:model=google_gemini-1.5-pro-001,model_deployment=stanfordhealthcare_gemini-1.5-pro\n",
      "Processing mtsamples_replicate:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing head_qa:language=en,category=None,model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing medcalc_bench:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing head_qa:language=en,category=None,model=meta_llama-3.3-70b-instruct,model_deployment=stanfordhealthcare_llama-3.3-70b-instruct\n",
      "Processing race_based_med:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing race_based_med:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing mtsamples_procedures:model=microsoft_phi-3.5-mini-instruct\n",
      "Processing mtsamples_procedures:model=openai_gpt-4o,model_deployment=stanfordhealthcare_gpt-4o-2024-05-13\n",
      "Processing medcalc_bench:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n",
      "Processing race_based_med:model=openai_gpt-4o-mini,model_deployment=stanfordhealthcare_gpt-4o-mini-2024-07-18\n"
     ]
    }
   ],
   "source": [
    "model_runs: dict[str, ModelRun] = {}\n",
    "for helm_run_folder in os.listdir(MEDHELM_RUN_PATH):\n",
    "    is_target_dataset = [\n",
    "        helm_run_folder.startswith(candidate) for candidate in TARGET_DATASETS\n",
    "    ]\n",
    "    if not any(is_target_dataset):\n",
    "        continue\n",
    "\n",
    "    helm_run_path = os.path.join(MEDHELM_RUN_PATH, helm_run_folder)\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {helm_run_folder}\")\n",
    "        model_run: ModelRun = process_helm_run(helm_run_path)\n",
    "        metrics_results = process_helm_metrics(helm_run_path)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    medbench_metrics_results = {\n",
    "        \"original_run\": {\n",
    "            \"metrics_type\": \"helm\",\n",
    "            \"model_run\": model_run.to_json(),\n",
    "        },\n",
    "        \"metrics_results\": metrics_results,\n",
    "        \"processed_at\": datetime.now(timezone.utc).isoformat(),\n",
    "    }\n",
    "\n",
    "    with open(\n",
    "        os.path.join(OUTPUT_PATH, f\"{helm_run_folder}.json\"), \"w+\"\n",
    "    ) as metrics_results_file:\n",
    "        json.dump(medbench_metrics_results, metrics_results_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data for Arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from medbench.datasets import CORRECT_TAG\n",
    "from medbench.models import ModelRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point to where the transformed data from MedHelm is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set it to OUTPUT_PATH/func_output\n",
    "MEDHELM_METRICS_PATH: str = \"../../data/medhelm/medbench_connection/medhelm-mixed/func_output\"\n",
    "\n",
    "MEDBENCH_ARENA_DATASET_TARGET_PATH: str = \"../../data/medhelm/medbench_connection/medhelm-mixed/arena/datasets\"\n",
    "MEDBENCH_ARENA_METRICS_TARGET_PATH: str = \"../../data/medhelm/medbench_connection/medhelm-mixed/arena/metrics\"\n",
    "\n",
    "if not os.path.exists(MEDHELM_METRICS_PATH):\n",
    "    os.makedirs(MEDHELM_METRICS_PATH)\n",
    "if not os.path.exists(MEDBENCH_ARENA_DATASET_TARGET_PATH):\n",
    "    os.makedirs(MEDBENCH_ARENA_DATASET_TARGET_PATH)\n",
    "if not os.path.exists(MEDBENCH_ARENA_METRICS_TARGET_PATH):\n",
    "    os.makedirs(MEDBENCH_ARENA_METRICS_TARGET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for getting dataset names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_id(medhelm_run_id: str, dataset_metadata: list[str] = [\"category\", \"subset\"]) -> str:\n",
    "    run_id_split = medhelm_run_id.split(\":\")\n",
    "    dataset_name = run_id_split[0]\n",
    "    metadata_str = \"\"\n",
    "    if len(run_id_split) == 2:\n",
    "        dataset_name, metadata_str = run_id_split\n",
    "    elif len(run_id_split) > 2:\n",
    "        dataset_name = run_id_split[0]\n",
    "        metadata_str = \",\".join(run_id_split[1:])\n",
    "\n",
    "    dataset_name_split = dataset_name.split(\",\")\n",
    "\n",
    "    dataset_id = dataset_name_split[0]\n",
    "    if len(dataset_name_split) > 1:\n",
    "        metadata_str += \",\" + \",\".join(dataset_name_split[1:])\n",
    "    \n",
    "    metadata = metadata_str.split(\",\")\n",
    "    # Sort the metadata to ensure consistent ordering\n",
    "    metadata.sort()\n",
    "    for data in metadata:\n",
    "        kv_split = data.split(\"=\")\n",
    "        if len(kv_split) != 2:\n",
    "            continue\n",
    "        key, value = kv_split \n",
    "        if key in dataset_metadata:\n",
    "            dataset_id += f\"-{value}\"\n",
    "    \n",
    "    return dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load transformed `ModelRun` objects and transform data into `jsonl` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map file names to model names\n",
    "clinical_tasks_metrics: dict = {}\n",
    "clinical_tasks_data: dict[str, list] = {}\n",
    "for model_run_filename in os.listdir(MEDHELM_METRICS_PATH):\n",
    "    model_run_path = os.path.join(MEDHELM_METRICS_PATH, model_run_filename)\n",
    "    if os.path.isdir(model_run_path):\n",
    "        continue\n",
    "\n",
    "    metrics_data: dict\n",
    "    with open(model_run_path, \"r\") as f:\n",
    "        metrics_data = json.load(f)\n",
    "\n",
    "    model_run = ModelRun.from_json(metrics_data[\"original_run\"][\"model_run\"])\n",
    "\n",
    "    clinical_task = get_dataset_id(model_run_filename)\n",
    "    if clinical_task.startswith(\"head_qa\"):\n",
    "        clinical_task = \"head_qa\"\n",
    "\n",
    "    # Store metrics\n",
    "    if clinical_task not in clinical_tasks_metrics:\n",
    "        clinical_tasks_metrics[clinical_task] = {}\n",
    "    clinical_tasks_metrics[clinical_task][model_run.model.name] = metrics_data[\n",
    "        \"metrics_results\"\n",
    "    ][\"aggregated_metrics\"]\n",
    "\n",
    "    # Store run data\n",
    "    if clinical_task not in clinical_tasks_data:\n",
    "        clinical_tasks_data[clinical_task] = {}\n",
    "\n",
    "    arena_dataset_data: list[dict] = []\n",
    "    # Sort instances by their id and results by their input_id before zipping\n",
    "    instances_sorted = sorted(model_run.dataset.instances, key=lambda inst: inst.id)\n",
    "    results_sorted = sorted(model_run.results, key=lambda res: res.input_id)\n",
    "\n",
    "    if len(instances_sorted) != len(results_sorted):\n",
    "        raise ValueError(f\"Length mismatch between instances ({len(instances_sorted)}) and results ({len(results_sorted)}) for {clinical_task}\")\n",
    "\n",
    "    for input, output in zip(instances_sorted, results_sorted):\n",
    "        if input.id != output.input_id:\n",
    "            raise ValueError(f\"ID mismatch after sorting: instance {input.id} != result {output.input_id} in {clinical_task}\")\n",
    "\n",
    "        correct_answer: str = None\n",
    "        for ref in input.references:\n",
    "            if CORRECT_TAG not in ref.tags:\n",
    "                continue\n",
    "\n",
    "            # This assume a single output was given\n",
    "            ref_content = ref.output.content[0]\n",
    "            if ref_content.metadata and \"multiple_choice_letter\" in ref_content.metadata:\n",
    "                correct_answer = ref_content.metadata[\"multiple_choice_letter\"]\n",
    "            else:\n",
    "                # This supports multiple outputs.\n",
    "                correct_answer = ref.output.get_text()\n",
    "            break\n",
    "\n",
    "        if correct_answer is None:\n",
    "            raise ValueError(\n",
    "                f\"Could not find correct answer for instance {input.id} in dataset {clinical_task}\"\n",
    "            )\n",
    "\n",
    "        arena_dataset_data.append(\n",
    "            {\n",
    "                \"id\": input.id,\n",
    "                \"input\": input.input.get_text(),\n",
    "                \"output\": output.completions.get_text(),\n",
    "                \"references\": correct_answer,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # print(f\"{clinical_task} {model_run.model.name} {len(arena_dataset_data)} - {model_run_filename}\")\n",
    "    clinical_tasks_data[clinical_task][model_run.model.name] = arena_dataset_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving metrics data\n",
      "Saving dataset data\n",
      "\tSaving ehr_sql dataset data\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-2.0-flash-001 Index(['id', 'input', 'google/gemini-2.0-flash-001', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-7-sonnet-20250219 Index(['id', 'input', 'anthropic/claude-3-7-sonnet-20250219', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-5-sonnet-20241022 Index(['id', 'input', 'anthropic/claude-3-5-sonnet-20241022', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o3-mini-2025-01-31 Index(['id', 'input', 'openai/o3-mini-2025-01-31', 'references'], dtype='object')\n",
      "\t\tMerging with deepseek-ai/deepseek-r1-hide-reasoning Index(['id', 'input', 'deepseek-ai/deepseek-r1-hide-reasoning', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 1000 rows for ehr_sql dataset\n",
      "\tSaving medec dataset data\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-7-sonnet-20250219 Index(['id', 'input', 'anthropic/claude-3-7-sonnet-20250219', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-2.0-flash-001 Index(['id', 'input', 'google/gemini-2.0-flash-001', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o3-mini-2025-01-31 Index(['id', 'input', 'openai/o3-mini-2025-01-31', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-5-sonnet-20241022 Index(['id', 'input', 'anthropic/claude-3-5-sonnet-20241022', 'references'], dtype='object')\n",
      "\t\tMerging with deepseek-ai/deepseek-r1-hide-reasoning Index(['id', 'input', 'deepseek-ai/deepseek-r1-hide-reasoning', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 597 rows for medec dataset\n",
      "\tSaving medcalc_bench dataset data\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-5-sonnet-20241022 Index(['id', 'input', 'anthropic/claude-3-5-sonnet-20241022', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-2.0-flash-001 Index(['id', 'input', 'google/gemini-2.0-flash-001', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with deepseek-ai/deepseek-r1-hide-reasoning Index(['id', 'input', 'deepseek-ai/deepseek-r1-hide-reasoning', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-7-sonnet-20250219 Index(['id', 'input', 'anthropic/claude-3-7-sonnet-20250219', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 190420 rows for medcalc_bench dataset\n",
      "\tSaving medbullets dataset data\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-5-sonnet-20241022 Index(['id', 'input', 'anthropic/claude-3-5-sonnet-20241022', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o3-mini-2025-01-31 Index(['id', 'input', 'openai/o3-mini-2025-01-31', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with deepseek-ai/deepseek-r1-hide-reasoning Index(['id', 'input', 'deepseek-ai/deepseek-r1-hide-reasoning', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-2.0-flash-001 Index(['id', 'input', 'google/gemini-2.0-flash-001', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-7-sonnet-20250219 Index(['id', 'input', 'anthropic/claude-3-7-sonnet-20250219', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 20768 rows for medbullets dataset\n",
      "\tSaving pubmed_qa dataset data\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-7-sonnet-20250219 Index(['id', 'input', 'anthropic/claude-3-7-sonnet-20250219', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o3-mini-2025-01-31 Index(['id', 'input', 'openai/o3-mini-2025-01-31', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with deepseek-ai/deepseek-r1-hide-reasoning Index(['id', 'input', 'deepseek-ai/deepseek-r1-hide-reasoning', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-2.0-flash-001 Index(['id', 'input', 'google/gemini-2.0-flash-001', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-5-sonnet-20241022 Index(['id', 'input', 'anthropic/claude-3-5-sonnet-20241022', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 1000 rows for pubmed_qa dataset\n",
      "\tSaving aci_bench dataset data\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with deepseek-ai/deepseek-r1 Index(['id', 'input', 'deepseek-ai/deepseek-r1', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 120 rows for aci_bench dataset\n",
      "\t\t- Columns with NaN (1): - deepseek-ai/deepseek-r1 (100)\n",
      "\t\t- Columns without NaN (9): id, input, openai/gpt-4o-mini, references, google/gemini-1.5-pro-001, qwen/qwen2.5-7b-instruct, openai/gpt-4o, microsoft/phi-3.5-mini-instruct, meta/llama-3.3-70b-instruct\n",
      "\n",
      "\tSaving mtsamples_procedures dataset data\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 1089 rows for mtsamples_procedures dataset\n",
      "\t\t- Columns with NaN (7): - id (97), - meta/llama-3.3-70b-instruct (97), - references (97), - microsoft/phi-3.5-mini-instruct (97), - openai/gpt-4o (97), - qwen/qwen2.5-7b-instruct (97), - google/gemini-1.5-pro-001 (97)\n",
      "\t\t- Columns without NaN (2): input, openai/gpt-4o-mini\n",
      "\n",
      "\tSaving medhallu dataset data\n",
      "\t\tMerging with google/gemini-2.0-flash-001 Index(['id', 'input', 'google/gemini-2.0-flash-001', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-7-sonnet-20250219 Index(['id', 'input', 'anthropic/claude-3-7-sonnet-20250219', 'references'], dtype='object')\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-5-sonnet-20241022 Index(['id', 'input', 'anthropic/claude-3-5-sonnet-20241022', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o3-mini-2025-01-31 Index(['id', 'input', 'openai/o3-mini-2025-01-31', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 1000 rows for medhallu dataset\n",
      "\tSaving head_qa dataset data\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o1 Index(['id', 'input', 'openai/o1', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-2.0-flash-001 Index(['id', 'input', 'google/gemini-2.0-flash-001', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o3-mini Index(['id', 'input', 'openai/o3-mini', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-7-sonnet-20250219 Index(['id', 'input', 'anthropic/claude-3-7-sonnet-20250219', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-5-sonnet-20241022 Index(['id', 'input', 'anthropic/claude-3-5-sonnet-20241022', 'references'], dtype='object')\n",
      "\t\tMerging with deepseek-ai/deepseek-r1-hide-reasoning Index(['id', 'input', 'deepseek-ai/deepseek-r1-hide-reasoning', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct-turbo Index(['id', 'input', 'meta/llama-3.3-70b-instruct-turbo', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o3-mini-2025-01-31 Index(['id', 'input', 'openai/o3-mini-2025-01-31', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 1000 rows for head_qa dataset\n",
      "\tSaving medication_qa dataset data\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 19935 rows for medication_qa dataset\n",
      "\tSaving race_based_med dataset data\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-2.0-flash-001 Index(['id', 'input', 'google/gemini-2.0-flash-001', 'references'], dtype='object')\n",
      "\t\tMerging with openai/o3-mini-2025-01-31 Index(['id', 'input', 'openai/o3-mini-2025-01-31', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-7-sonnet-20250219 Index(['id', 'input', 'anthropic/claude-3-7-sonnet-20250219', 'references'], dtype='object')\n",
      "\t\tMerging with anthropic/claude-3-5-sonnet-20241022 Index(['id', 'input', 'anthropic/claude-3-5-sonnet-20241022', 'references'], dtype='object')\n",
      "\t\tMerging with deepseek-ai/deepseek-r1-hide-reasoning Index(['id', 'input', 'deepseek-ai/deepseek-r1-hide-reasoning', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\t\tMerging with qwen/qwen2.5-7b-instruct Index(['id', 'input', 'qwen/qwen2.5-7b-instruct', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 334 rows for race_based_med dataset\n",
      "\t\t- Columns with NaN (13): - id (167), - google/gemini-1.5-pro-001 (167), - references (167), - openai/gpt-4o-mini (167), - microsoft/phi-3.5-mini-instruct (167), - google/gemini-2.0-flash-001 (167), - openai/o3-mini-2025-01-31 (167), - anthropic/claude-3-7-sonnet-20250219 (167), - anthropic/claude-3-5-sonnet-20241022 (167), - deepseek-ai/deepseek-r1-hide-reasoning (167), - meta/llama-3.3-70b-instruct (167), - openai/gpt-4o (167), - qwen/qwen2.5-7b-instruct (167)\n",
      "\t\t- Columns without NaN (1): input\n",
      "\n",
      "\tSaving mtsamples_replicate dataset data\n",
      "\t\tMerging with microsoft/phi-3.5-mini-instruct Index(['id', 'input', 'microsoft/phi-3.5-mini-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o-mini Index(['id', 'input', 'openai/gpt-4o-mini', 'references'], dtype='object')\n",
      "\t\tMerging with google/gemini-1.5-pro-001 Index(['id', 'input', 'google/gemini-1.5-pro-001', 'references'], dtype='object')\n",
      "\t\tMerging with meta/llama-3.3-70b-instruct Index(['id', 'input', 'meta/llama-3.3-70b-instruct', 'references'], dtype='object')\n",
      "\t\tMerging with openai/gpt-4o Index(['id', 'input', 'openai/gpt-4o', 'references'], dtype='object')\n",
      "\n",
      "\t\tStats:\n",
      "\t\t- Merged 1636 rows for mtsamples_replicate dataset\n",
      "\t\t- Columns with NaN (7): - id (388), - qwen/qwen2.5-7b-instruct (388), - references (388), - microsoft/phi-3.5-mini-instruct (388), - google/gemini-1.5-pro-001 (388), - meta/llama-3.3-70b-instruct (388), - openai/gpt-4o (388)\n",
      "\t\t- Columns without NaN (2): input, openai/gpt-4o-mini\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the metrics data\n",
    "print(\"Saving metrics data\")\n",
    "for clinical_task in clinical_tasks_metrics:\n",
    "    with open(\n",
    "        os.path.join(MEDBENCH_ARENA_METRICS_TARGET_PATH, f\"{clinical_task}.json\"),\n",
    "        \"w+\",\n",
    "    ) as metrics_results_file:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"metrics\": clinical_tasks_metrics[clinical_task],\n",
    "                \"clinical_task\": clinical_task,\n",
    "            },\n",
    "            metrics_results_file,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "# Save the dataset data\n",
    "print(\"Saving dataset data\")\n",
    "for clinical_task in clinical_tasks_data:\n",
    "    print(f\"\\tSaving {clinical_task} dataset data\")\n",
    "    merged_df: pd.DataFrame = None\n",
    "    for model_name in clinical_tasks_data[clinical_task]:\n",
    "        model_df = pd.DataFrame(clinical_tasks_data[clinical_task][model_name])\n",
    "        model_df.rename(\n",
    "            columns={\n",
    "                \"output\": model_name,\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        if merged_df is None:\n",
    "            merged_df = model_df\n",
    "            continue\n",
    "\n",
    "        print(\"\\t\\tMerging with\", model_name, model_df.columns)\n",
    "        # # Check if model_df[\"input\"] and model_df[\"references\"] are the same as in merged_df\n",
    "        # if not model_df[\"input\"].equals(merged_df[\"input\"]):\n",
    "        #     print(\"\\t\\t\\tWarning: input columns do not match between models, skipping input column merge\")\n",
    "        # if not model_df[\"references\"].equals(merged_df[\"references\"]):\n",
    "        #     print(\"\\t\\t\\tWarning: references columns do not match between models, skipping references column merge\")\n",
    "\n",
    "        # model_df.drop(columns=[\"input\", \"references\"], inplace=True)\n",
    "        model_df.drop(columns=[\"id\", \"references\"], inplace=True)\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            model_df,\n",
    "            how=\"outer\",\n",
    "            # NOTE: We merge on \"input\" because MedHelm ids may not be consistent across (version) runs.\n",
    "            on=[\"input\"],\n",
    "            # suffixes=(None, f\"_{model_name}\"),\n",
    "        )\n",
    "\n",
    "    print(\"\\n\\t\\tStats:\")\n",
    "    print(f\"\\t\\t- Merged {len(merged_df)} rows for {clinical_task} dataset\")\n",
    "    \n",
    "    # Log columns with and without NaN values\n",
    "    if merged_df.isna().any().any():\n",
    "        na_counts = merged_df.isna().sum()\n",
    "        cols_with_na_counts = [f\"- {col} ({int(cnt)})\" for col, cnt in na_counts.items() if cnt > 0]\n",
    "        cols_without_na = [col for col, cnt in na_counts.items() if cnt == 0]\n",
    "\n",
    "        print(f\"\\t\\t- Columns with NaN ({len(cols_with_na_counts)}): {', '.join(cols_with_na_counts) if cols_with_na_counts else 'None'}\")\n",
    "        print(f\"\\t\\t- Columns without NaN ({len(cols_without_na)}): {', '.join(cols_without_na) if cols_without_na else 'None'}\\n\")\n",
    "\n",
    "    merged_df.drop(columns=[\"id\"], inplace=True)\n",
    "    merged_df.to_json(\n",
    "        os.path.join(MEDBENCH_ARENA_DATASET_TARGET_PATH, f\"{clinical_task}.jsonl\"),\n",
    "        orient=\"records\",\n",
    "        lines=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log clinical tasks with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in ehr_sql\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\treferences: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\tgoogle/gemini-2.0-flash-001: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\tanthropic/claude-3-7-sonnet-20250219: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\tanthropic/claude-3-5-sonnet-20241022: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\topenai/o3-mini-2025-01-31: No missing values\n",
      "\tdeepseek-ai/deepseek-r1-hide-reasoning: No missing values\n",
      "\n",
      "Missing values in medec\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\treferences: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\tanthropic/claude-3-7-sonnet-20250219: No missing values\n",
      "\tgoogle/gemini-2.0-flash-001: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\topenai/o3-mini-2025-01-31: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\tanthropic/claude-3-5-sonnet-20241022: No missing values\n",
      "\tdeepseek-ai/deepseek-r1-hide-reasoning: No missing values\n",
      "\n",
      "Missing values in medcalc_bench\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\topenai/o3-mini-2025-01-31: No missing values\n",
      "\treferences: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\tanthropic/claude-3-5-sonnet-20241022: No missing values\n",
      "\tgoogle/gemini-2.0-flash-001: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\tdeepseek-ai/deepseek-r1-hide-reasoning: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\tanthropic/claude-3-7-sonnet-20250219: No missing values\n",
      "\n",
      "Missing values in medbullets\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\treferences: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\tanthropic/claude-3-5-sonnet-20241022: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\topenai/o3-mini-2025-01-31: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\tdeepseek-ai/deepseek-r1-hide-reasoning: No missing values\n",
      "\tgoogle/gemini-2.0-flash-001: No missing values\n",
      "\tanthropic/claude-3-7-sonnet-20250219: No missing values\n",
      "\n",
      "Missing values in pubmed_qa\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\treferences: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\tanthropic/claude-3-7-sonnet-20250219: No missing values\n",
      "\topenai/o3-mini-2025-01-31: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\tdeepseek-ai/deepseek-r1-hide-reasoning: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\tgoogle/gemini-2.0-flash-001: No missing values\n",
      "\tanthropic/claude-3-5-sonnet-20241022: No missing values\n",
      "\n",
      "Missing values in aci_bench\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\treferences: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\tdeepseek-ai/deepseek-r1: 100\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\n",
      "Missing values in mtsamples_procedures\n",
      "\tid: No missing values\n",
      "\tinput: 66\n",
      "\tmeta/llama-3.3-70b-instruct: 66\n",
      "\treferences: 66\n",
      "\tmicrosoft/phi-3.5-mini-instruct: 66\n",
      "\topenai/gpt-4o: 66\n",
      "\tqwen/qwen2.5-7b-instruct: 66\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: 66\n",
      "\n",
      "Missing values in medhallu\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\tdeepseek-ai/deepseek-r1-hide-reasoning: No missing values\n",
      "\treferences: No missing values\n",
      "\tgoogle/gemini-2.0-flash-001: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\tanthropic/claude-3-7-sonnet-20250219: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\tanthropic/claude-3-5-sonnet-20241022: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\topenai/o3-mini-2025-01-31: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\n",
      "Missing values in head_qa\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\treferences: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\topenai/o1: No missing values\n",
      "\tgoogle/gemini-2.0-flash-001: No missing values\n",
      "\topenai/o3-mini: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\tanthropic/claude-3-7-sonnet-20250219: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\tanthropic/claude-3-5-sonnet-20241022: No missing values\n",
      "\tdeepseek-ai/deepseek-r1-hide-reasoning: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct-turbo: No missing values\n",
      "\topenai/o3-mini-2025-01-31: No missing values\n",
      "\n",
      "Missing values in medication_qa\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\treferences: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\n",
      "Missing values in race_based_med\n",
      "\tid: No missing values\n",
      "\tinput: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: No missing values\n",
      "\treferences: No missing values\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\tmicrosoft/phi-3.5-mini-instruct: No missing values\n",
      "\tgoogle/gemini-2.0-flash-001: No missing values\n",
      "\topenai/o3-mini-2025-01-31: No missing values\n",
      "\tanthropic/claude-3-7-sonnet-20250219: No missing values\n",
      "\tanthropic/claude-3-5-sonnet-20241022: No missing values\n",
      "\tdeepseek-ai/deepseek-r1-hide-reasoning: No missing values\n",
      "\tmeta/llama-3.3-70b-instruct: No missing values\n",
      "\topenai/gpt-4o: No missing values\n",
      "\tqwen/qwen2.5-7b-instruct: No missing values\n",
      "\n",
      "Missing values in mtsamples_replicate\n",
      "\tid: No missing values\n",
      "\tinput: 349\n",
      "\tqwen/qwen2.5-7b-instruct: 349\n",
      "\treferences: 349\n",
      "\tmicrosoft/phi-3.5-mini-instruct: 349\n",
      "\topenai/gpt-4o-mini: No missing values\n",
      "\tgoogle/gemini-1.5-pro-001: 349\n",
      "\tmeta/llama-3.3-70b-instruct: 349\n",
      "\topenai/gpt-4o: 349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clinical_task = \"aci_bench\"\n",
    "for clinical_task in clinical_tasks_data:\n",
    "    merged_df: pd.DataFrame = None\n",
    "    for model_name in clinical_tasks_data[clinical_task]:\n",
    "        model_df = pd.DataFrame(clinical_tasks_data[clinical_task][model_name])\n",
    "        model_df.rename(\n",
    "            columns={\n",
    "                \"output\": model_name,\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        if merged_df is None:\n",
    "            merged_df = model_df\n",
    "            continue\n",
    "\n",
    "        model_df.drop(columns=[\"input\", \"references\"], inplace=True)\n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            model_df,\n",
    "            how=\"outer\",\n",
    "            on=[\"id\"],\n",
    "        )\n",
    "\n",
    "    # Print column names that contain na values, along with the number of na values\n",
    "    print(f\"Missing values in {clinical_task}\")\n",
    "    for column in merged_df.columns:\n",
    "        if merged_df[column].isna().any():\n",
    "            print(f\"\\t{column}: {merged_df[column].isna().sum()}\")\n",
    "        else:\n",
    "            print(f\"\\t{column}: No missing values\")\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medbench-arena-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
