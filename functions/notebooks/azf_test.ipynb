{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from medbench.evaluators import (\n",
    "    EvaluatorRunner,\n",
    "    SummaryEvaluatorRunner,\n",
    "    MultimodalEvaluatorRunner,\n",
    "    ABEvaluatorRunner,\n",
    ")\n",
    "from medbench.metrics import (\n",
    "    calculate_exact_match_metrics,\n",
    "    calculate_image_metrics,\n",
    "    calculate_summarization_metrics,\n",
    ")\n",
    "from medbench.models import Model, ModelRegistry, ModelRun, OpenAIReasoningModel, Runner\n",
    "from medbench.config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/azf/metrics\"\n",
    "METRICS_INPUT_PATH = os.path.join(\n",
    "    DATA_PATH,\n",
    "    \"vqarad_gpt4o_eval_metrics_input.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_type: str\n",
    "model_run: ModelRun\n",
    "with open(METRICS_INPUT_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "metrics_type = data.get(\"metrics_type\")\n",
    "model_run = ModelRun.from_json(data.get(\"model_run\"))\n",
    "\n",
    "model_run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run.model.name, metrics_type, f\"Instances to evaluate: {len(model_run.results)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics: list[dict[str, float]]\n",
    "if metrics_type == \"exact_match\":\n",
    "    metrics = calculate_exact_match_metrics(model_run)\n",
    "elif metrics_type == \"summarization\":\n",
    "    metrics = calculate_summarization_metrics(model_run)\n",
    "elif metrics_type == \"image_match\":\n",
    "    metrics = calculate_image_metrics(model_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model as a judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../\"\n",
    "\n",
    "SAMPLE_FULL_VALIDATION_JOB_JSON = os.path.join(\n",
    "    DATA_PATH, \"sample_full_validation_job.json\"\n",
    ")\n",
    "\n",
    "SAMPLE_AB_TESTING_JOB_JSON = os.path.join(\n",
    "    DATA_PATH, \"sample_ab_testing_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import attrs\n",
    "from medbench.datasets import (\n",
    "    Data,\n",
    "    Dataset,\n",
    "    EMediaObjectType,\n",
    "    Instance,\n",
    "    MediaObject,\n",
    ")\n",
    "\n",
    "\n",
    "async def run_summary_evaluator(model_run, llm_evaluator, questions_generator_runner=None, output_instructions=\"\"):\n",
    "    \"\"\"\n",
    "    Run SummaryEvaluatorRunner.\n",
    "    \n",
    "    Args:\n",
    "        model_run: The model run to evaluate\n",
    "        llm_evaluator: The LLM evaluator model\n",
    "        questions_generator_runner: Optional pre-existing questions generator\n",
    "        output_instructions: Optional output specifications to inject\n",
    "        \n",
    "    Returns:\n",
    "        SummaryEvaluatorRunner: The completed evaluator instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize SummaryEvaluatorRunner\n",
    "        kwargs = {}\n",
    "        \n",
    "        # Use provided questions generator if available\n",
    "        if questions_generator_runner is not None:\n",
    "            kwargs[\"questions_generator_runner\"] = questions_generator_runner\n",
    "\n",
    "        # Inject output instructions if provided\n",
    "        if output_instructions:\n",
    "            kwargs[\"output_specs_prompt\"] = output_instructions\n",
    "\n",
    "        evaluator = SummaryEvaluatorRunner(\n",
    "            predictions_model_run=model_run,\n",
    "            evaluator=llm_evaluator,\n",
    "            skip_errors=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Run summary evaluation\n",
    "        await evaluator.evaluate()\n",
    "\n",
    "        # Return the evaluator itself\n",
    "        return evaluator\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in Summary evaluation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "async def _process_model_run_async(\n",
    "    model_run, llm_evaluator, questions_generator_runner=None, output_instructions=\"\"\n",
    "):\n",
    "    \"\"\"Helper function to process model run using SummaryEvaluatorRunner with injected output instructions.\"\"\"\n",
    "    try:\n",
    "        evaluator = await run_summary_evaluator(\n",
    "            model_run, llm_evaluator, questions_generator_runner, output_instructions\n",
    "        )\n",
    "        # Extract the evaluation result text\n",
    "        return evaluator.evaluator_runner._model_run.results[0].completions.get_text()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in Summary evaluation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "async def _process_ab_testing_async(model_run, llm_evaluator, output_instructions):\n",
    "    \"\"\"Helper function to process A/B testing using SummaryEvaluatorRunner + MultimodalEvaluatorRunner.\"\"\"\n",
    "    try:\n",
    "        # Step 1: Run vanilla SummaryEvaluatorRunner for each model separately\n",
    "        model_runs = []\n",
    "        questions_generator_runner = None  # Placeholder for questions generator if needed\n",
    "        for i, result in enumerate(model_run.results):\n",
    "            # Create a single-model run for each result\n",
    "            single_model_run = attrs.evolve(\n",
    "                model_run, id=f\"{model_run.id}_model_{i}\", results=[result]\n",
    "            )\n",
    "\n",
    "            # Process with vanilla SummaryEvaluatorRunner using the shared function\n",
    "            evaluator = await run_summary_evaluator(\n",
    "                single_model_run, llm_evaluator, questions_generator_runner\n",
    "            )\n",
    "\n",
    "            questions_generator_runner = evaluator.questions_generator_runner\n",
    "\n",
    "            # Extract the evaluator runner's model run for AB comparison\n",
    "            model_runs.append(evaluator.evaluator_runner._model_run)\n",
    "\n",
    "        # Step 2: Use ABEvaluatorRunner to compare the outputs according to output_instructions\n",
    "        comparison_runner = ABEvaluatorRunner(\n",
    "            predictions_model_run=model_runs[0],\n",
    "            predictions_model_run_b=model_runs[1],\n",
    "            evaluator=llm_evaluator,\n",
    "            output_specs_prompt=output_instructions,\n",
    "        )\n",
    "\n",
    "        await comparison_runner.evaluate()\n",
    "        return comparison_runner\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in A/B testing evaluation: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAMPLE_AB_TESTING_JOB_JSON, \"r\") as f:\n",
    "    request_data = json.load(f)\n",
    "\n",
    "# Parse the model run data\n",
    "model_run_data = request_data.get(\"model_run\", {})\n",
    "model_run = ModelRun.from_json(model_run_data)\n",
    "\n",
    "# Extract output instructions for injection into evaluator\n",
    "output_instructions = request_data.get(\"output_instructions\", \"\")\n",
    "\n",
    "# Initialize OpenAI model for Summary evaluation\n",
    "# Use MedBench config settings with fallback to environment variables\n",
    "llm_evaluator = OpenAIReasoningModel(\n",
    "    name=settings.azure_openai_deployment,\n",
    "    version=settings.azure_openai_version,\n",
    "    endpoint=settings.azure_openai_endpoint,\n",
    "    api_key=settings.azure_openai_api_key,\n",
    "    vision_enabled=False,\n",
    "    system_prompt=\"\",  # Prompts are defined by the evaluator runner\n",
    "    max_tokens=40000,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Check if this is A/B testing (Arena experiment)\n",
    "is_ab_testing = len(model_run.results) > 1\n",
    "\n",
    "output_instructions, model_run.id, is_ab_testing, model_run.model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = await _process_ab_testing_async(model_run, llm_evaluator, output_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mr.evaluator_runner._model_run.dataset.instances[0].input.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.predictions_model_run.results[0].completions.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.predictions_model_run_b.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {}\n",
    "if output_instructions:\n",
    "    kwargs[\"output_specs_prompt\"] = output_instructions\n",
    "\n",
    "evaluator = SummaryEvaluatorRunner(\n",
    "    predictions_model_run=model_run,\n",
    "    evaluator=llm_evaluator,\n",
    "    skip_errors=True,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluator_runner._model_run.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drill down debugging of evaluation workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check original dataset's outputs - that's what we will be evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.predictions_model_run.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check questions' dataset, the first step of the `SummaryEvaluator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.questions_generator_runner._model_run.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the `results` key, we generated questions correctly, so let's check the second step of the `SummaryEvaluator`: the answer generation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.answerer_runner._model_run.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, something is wrong with the answer generation step. First, we have no input for this step (empty `instance`, and thus no answers were generated (empty `results`). The `SummaryEvaluator`, after generating questions, should be prepare the answer generation input in two steps:\n",
    "\n",
    "```python\n",
    "# ...\n",
    "\n",
    "questions: List[Instance] = self._process_triplet_output(\n",
    "    self.questions_generator_runner._model_run.results,\n",
    ")\n",
    "\n",
    "# ...\n",
    "instances=self._prepare_summary_questions_instances(\n",
    "    questions, self.predictions_model_run.results\n",
    ")\n",
    "\n",
    "# ...\n",
    "```\n",
    "\n",
    "So let's look into these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = evaluator._process_triplet_output(\n",
    "    evaluator.questions_generator_runner._model_run.results,\n",
    ")\n",
    "\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator._prepare_summary_questions_instances(\n",
    "    questions, evaluator.predictions_model_run.results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drill down into `_prepare_summary_questions_instances`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic function's params:\n",
    "# questions = questions\n",
    "summaries = evaluator.predictions_model_run.results\n",
    "questions_header: str = \"QUESTIONS:\\n\\n\"\n",
    "summaries_header: str = \"\\n\\nAI SYSTEM SUMMARY:\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_questions_map = {}\n",
    "for question in questions:\n",
    "    if question.id not in summary_questions_map:\n",
    "        summary_questions_map[question.id] = []\n",
    "    summary_questions_map[question.id].append(question)\n",
    "\n",
    "summary_questions_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic loop\n",
    "summary = summaries[0]\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop conditions\n",
    "summary.error is not None, summary.input_id not in summary_questions_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.input_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_questions_map: Dict[str, List[Instance]] = {}\n",
    "for question in questions:\n",
    "    if question.id not in summary_questions_map:\n",
    "        summary_questions_map[question.id] = []\n",
    "    summary_questions_map[question.id].append(question)\n",
    "\n",
    "prepared_instances: List[Instance] = []\n",
    "for summary in summaries:\n",
    "    if summary.error is not None:\n",
    "        logging.debug(\n",
    "            \"Cannot prepare summary questions for evaluation. \"\n",
    "            f\"Skipping instance {summary.input_id} due to error: {summary.error}\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    if summary.input_id not in summary_questions_map:\n",
    "        logging.debug(\n",
    "            f\"Skipping instance {summary.input_id} because no questions were generated.\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Questions and references are in the same format, one line per entry.\n",
    "    questions_text = \"\\n\".join(\n",
    "        [\n",
    "            q.input.get_text().strip()\n",
    "            for q in summary_questions_map[summary.input_id]\n",
    "        ]\n",
    "    )\n",
    "    questions_references = \"\\n\".join(\n",
    "        [\n",
    "            r.output.get_text().strip()\n",
    "            for q in summary_questions_map[summary.input_id]\n",
    "            for r in q.references\n",
    "        ]\n",
    "    )\n",
    "    prepared_instances.append(\n",
    "        Instance(\n",
    "            id=summary.input_id,\n",
    "            input=Data.from_text(\n",
    "                data=(\n",
    "                    questions_header\n",
    "                    + questions_text\n",
    "                    + summaries_header\n",
    "                    + summary.completions.get_text()\n",
    "                )\n",
    "            ),\n",
    "            references=questions_references,\n",
    "            split=\"eval\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "return prepared_instances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medbench-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
