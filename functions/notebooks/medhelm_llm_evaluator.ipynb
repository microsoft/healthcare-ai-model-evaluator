{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16c8412",
   "metadata": {},
   "source": [
    "# LLM as a judge over MedHelm datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ae5b9",
   "metadata": {},
   "source": [
    "Current learnings:\n",
    "- Limitations on TPM\n",
    "- Limitations on output length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280005aa",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b0808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dba9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee0d1668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Environment variable BABELBENCH_AML_WORKSPACE_NAME not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from medbench.config import settings\n",
    "from medbench.datasets import Dataset\n",
    "from medbench.evaluators import MultimodalEvaluatorRunner, SummaryEvaluatorRunner\n",
    "from medbench.models import ModelOutput, ModelRun, SystemPromptModel, Runner\n",
    "from medbench.models.azureoai import OpenAIChatModel, OpenAIReasoningModel\n",
    "from medbench.models.cxrreportgen import CXRReportGenModel\n",
    "from medbench.utils import load_arena_data\n",
    "\n",
    "load_dotenv(\"../.env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013cb64",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605ad755",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "MEDHELM_DATA_PATH = os.path.join(DATA_PATH, \"metrics/medhelm\")\n",
    "MEDHELM_DATASETS_PATH = os.path.join(MEDHELM_DATA_PATH, \"arena/datasets/\")\n",
    "MEDHELM_METRICS_PATH = os.path.join(MEDHELM_DATA_PATH, \"arena/metrics/\")\n",
    "MEDHELM_MODEL_RUNS_PATH = os.path.join(MEDHELM_DATA_PATH, \"func_output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e21cf42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aci_bench', 'mtsamples', 'medication_qa', 'mtsamples_replicate'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freetext_model_runs: dict[str, list[ModelRun]] = {}\n",
    "for filename in os.listdir(MEDHELM_MODEL_RUNS_PATH):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(MEDHELM_MODEL_RUNS_PATH, filename), \"r\") as f:\n",
    "            model_run_data = json.load(f)\n",
    "\n",
    "            if \"bert_score\" not in model_run_data[\"metrics_results\"][\"aggregated_metrics\"]:\n",
    "                continue\n",
    "            \n",
    "            model_run = ModelRun.from_json(model_run_data[\"original_run\"][\"model_run\"])\n",
    "\n",
    "            if model_run.dataset.name not in freetext_model_runs:\n",
    "                freetext_model_runs[model_run.dataset.name] = []\n",
    "            freetext_model_runs[model_run.dataset.name].append(model_run)\n",
    "\n",
    "freetext_model_runs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5277a63",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ec416",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_OUTPUT_PATH = os.path.join(MEDHELM_DATA_PATH, \"medbench_evaluation/\")\n",
    "os.makedirs(EVALUATION_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc44888",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def batch_summarization_eval(\n",
    "    model_runs: list[ModelRun],\n",
    "    llm_evaluator: SystemPromptModel,\n",
    "    output_path: str,\n",
    "    questions_generator_runner: Runner = None,\n",
    "):\n",
    "    \"\"\"Batch evaluation of summarization tasks.\n",
    "    \n",
    "    Args:\n",
    "        model_runs (list[ModelRun]): List of model runs to evaluate.\n",
    "            Note that all model runs are expected to be from the **same** dataset.\n",
    "        llm_evaluator (SystemPromptModel): LLM evaluator to use for evaluation.\n",
    "    \"\"\"\n",
    "    summarization_evaluators = {}\n",
    "    for model_run in model_runs:\n",
    "        kwargs = {}\n",
    "        if questions_generator_runner is not None:\n",
    "            kwargs[\"questions_generator_runner\"] = questions_generator_runner\n",
    "        \n",
    "        evaluator = SummaryEvaluatorRunner(\n",
    "            predictions_model_run=model_run,\n",
    "            evaluator=llm_evaluator,\n",
    "            skip_errors=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        summarization_evaluators[model_run.model.name] = evaluator\n",
    "\n",
    "        await evaluator.evaluate()\n",
    "\n",
    "        if questions_generator_runner is None:\n",
    "            questions_generator_runner = evaluator.questions_generator_runner\n",
    "\n",
    "            with open(os.path.join(output_path, f\"{model_run.dataset.name}-questions.json\"), \"w+\") as f:\n",
    "                json.dump(evaluator.questions_generator_runner._model_run.to_json(), f, indent=2)\n",
    "        \n",
    "        model_name = model_run.model.name.replace(\"/\", \"-\")\n",
    "        with open(os.path.join(output_path, f\"{model_name}-answers.json\"), \"w+\") as f:\n",
    "            json.dump(evaluator.answerer_runner._model_run.to_json(), f, indent=2)\n",
    "        \n",
    "        with open(os.path.join(output_path, f\"{model_name}-summ.json\"), \"w+\") as f:\n",
    "            json.dump(evaluator.evaluator_runner._model_run.to_json(), f, indent=2)\n",
    "\n",
    "    return summarization_evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_text_evaluator = OpenAIReasoningModel(\n",
    "    name=settings.azure_openai_o3_deployment,\n",
    "    version=settings.azure_openai_o3_version,\n",
    "    endpoint=settings.azure_openai_o3_endpoint,\n",
    "    api_key=settings.azure_openai_o3_api_key,\n",
    "    vision_enabled=False,\n",
    "    # Prompts are defined by the evaluator runner.\n",
    "    system_prompt=\"\",\n",
    "    # Values from AI foundry playground\n",
    "    max_tokens=40000,\n",
    "    stop=None,\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "llm_text_evaluator.name, llm_text_evaluator.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06326abc",
   "metadata": {},
   "source": [
    "### ACI Bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c90bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACI_BENCH_OUTPUT_PATH = os.path.join(EVALUATION_OUTPUT_PATH, \"aci_bench/\")\n",
    "os.makedirs(ACI_BENCH_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_runs = freetext_model_runs[\"aci_bench\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7517152c",
   "metadata": {},
   "source": [
    "#### Summary evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6558d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACI_BENCH_SUMM_OUTPUT_PATH = os.path.join(ACI_BENCH_OUTPUT_PATH, \"summ/\")\n",
    "os.makedirs(ACI_BENCH_SUMM_OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d311a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_model_run = None\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(ACI_BENCH_SUMM_OUTPUT_PATH, \"aci_bench-questions.json\"), \"w+\") as f:\n",
    "        questions_model_run = ModelRun.from_json(json.load(f))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "questions_model_run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f6c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_bench_evaluators = await batch_summarization_eval(\n",
    "    model_runs=model_runs,\n",
    "    llm_evaluator=llm_text_evaluator,\n",
    "    output_path=ACI_BENCH_SUMM_OUTPUT_PATH,\n",
    "    questions_generator_runner=questions_model_run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af79536",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model_runs = {}\n",
    "for output_filename in os.listdir(ACI_BENCH_SUMM_OUTPUT_PATH):\n",
    "    if output_filename.endswith(\"summ.json\"):\n",
    "        with open(os.path.join(ACI_BENCH_SUMM_OUTPUT_PATH, output_filename), \"r\") as f:\n",
    "            model_run_data = json.load(f)\n",
    "            model_run = ModelRun.from_json(model_run_data)\n",
    "            summary_model_runs[output_filename] = model_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model_run in summary_model_runs.items():\n",
    "    results = \"\"\n",
    "    results += f\"Model: {model_name}\\n\"\n",
    "    results += f\"Results count: {len(model_run.results)}\\n\"\n",
    "    for res in model_run.results:\n",
    "        if res.completions is None:\n",
    "            continue\n",
    "        results += f\"First results: {res.completions.get_text()}\\n\"\n",
    "        results += \"\\n____________________________________________________________\\n\"\n",
    "\n",
    "    with open(os.path.join(ACI_BENCH_SUMM_OUTPUT_PATH, f\"aci_bench-{model_name}-results.txt\"), \"w+\") as f:\n",
    "        f.write(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9c8d2",
   "metadata": {},
   "source": [
    "#### TBFact evaluator\n",
    "\n",
    "This section runs the TBFact evaluator on the ACI Bench model runs and saves the results for further analysis and human review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f550b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACI_BENCH_TBFACT_OUTPUT_PATH = os.path.join(ACI_BENCH_OUTPUT_PATH, \"tbfact/\")\n",
    "ACI_BENCH_TBFACT_REFERENCE_FACTS_PATH = os.path.join(ACI_BENCH_OUTPUT_PATH, \"tbfact/reference_facts/reference_facts.json\")\n",
    "\n",
    "os.makedirs(ACI_BENCH_TBFACT_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(ACI_BENCH_TBFACT_REFERENCE_FACTS_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde57ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medbench.evaluators.tbfact.runner import TBFactEvaluatorRunner\n",
    "from medbench.models import SystemPromptModel, Runner\n",
    "\n",
    "tbfact_evaluators = {}\n",
    "for model_run in model_runs:\n",
    "    tbfact_evaluator = TBFactEvaluatorRunner(\n",
    "        predictions_model_run=model_run,\n",
    "        evaluator=llm_text_evaluator,\n",
    "        reference_facts_path=ACI_BENCH_TBFACT_REFERENCE_FACTS_PATH,\n",
    "    )\n",
    "    await tbfact_evaluator.evaluate()\n",
    "\n",
    "    if not os.path.exists(ACI_BENCH_TBFACT_REFERENCE_FACTS_PATH):\n",
    "        tbfact_evaluator.tbfact.save_reference_facts(ACI_BENCH_TBFACT_REFERENCE_FACTS_PATH)\n",
    "\n",
    "    tbfact_evaluators[model_run.model.name] = tbfact_evaluator\n",
    "\n",
    "    model_name = model_run.model.name.replace(\"/\", \"-\")\n",
    "    with open(os.path.join(ACI_BENCH_TBFACT_OUTPUT_PATH, f\"{model_name}-tbfact.json\"), \"w+\") as f:\n",
    "        json.dump(tbfact_evaluator.tbfact_evaluation_model_run.to_json(), f, indent=2)\n",
    "    \n",
    "    with open(os.path.join(ACI_BENCH_TBFACT_OUTPUT_PATH, f\"{model_name}-fact-extraction.json\"), \"w+\") as f:\n",
    "        json.dump(tbfact_evaluator.fact_extraction_model_run.to_json(), f, indent=2)\n",
    "    \n",
    "    with open(os.path.join(ACI_BENCH_TBFACT_OUTPUT_PATH, f\"{model_name}-entailment.json\"), \"w+\") as f:\n",
    "        json.dump(tbfact_evaluator.entailment_model_run.to_json(), f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45651c1a",
   "metadata": {},
   "source": [
    "The TBFact evaluation results are now saved in the output directory for each model. These can be further analyzed or loaded into MedBench Arena for human expert review."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medbench-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
