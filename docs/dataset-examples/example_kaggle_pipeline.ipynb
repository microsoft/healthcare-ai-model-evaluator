{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad2a881e",
   "metadata": {},
   "source": [
    "# Kaggle dataset-to-JSONL export template (Image)\n",
    "Use this notebook as a guided recipe for going from a Kaggle competition dataset to a tidy `.jsonl` asset ready for downstream ML tooling. It double-checks Python dependencies, validates your Kaggle API credentials, optionally searches for the dataset slug, downloads and extracts the archive, and finally base64-encodes every image with metadata so you can store the samples alongside labels in a single JSON Lines file under `data/`. Run the cells in order whenever you need to refresh the export or adapt it to a different Kaggle dataset.\n",
    "\n",
    "The dataset(s) in this example are used for demonstration purposes only. Microsoft does not endorse them specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2418c8",
   "metadata": {},
   "source": [
    "## Workflow Overview\n",
    "1. Check and install the Python dependencies we need (Kaggle API, Pillow, tqdm).\n",
    "2. Configure working directories and reusable constants for this run.\n",
    "3. Validate the Kaggle API credentials so authenticated downloads work.\n",
    "4. Discover the dataset reference on Kaggle (or use a manual override, if supplied).\n",
    "5. Download the compressed archive with all images.\n",
    "6. Extract the images into a clean workspace.\n",
    "7. Encode each image as base64, gather metadata, and write a consolidated JSONL file.\n",
    "8. Inspect the JSONL output to confirm counts and schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c44668",
   "metadata": {},
   "source": [
    "## 1. Install / verify dependencies\n",
    "The Kaggle CLI plus Pillow (for reading image metadata) and tqdm (for progress bars) are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "required_packages = [\"kaggle\", \"Pillow\", \"tqdm\"]\n",
    "print(f'Ensuring required packages are installed: {required_packages}')\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', *required_packages], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55265855",
   "metadata": {},
   "source": [
    "## 2. Configure paths and constants\n",
    "Adjust `DATASET_REF_OVERRIDE` if you already know the exact Kaggle dataset slug (e.g., `owner/dataset-name`). Otherwise the next step will search for it using the provided text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ce3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('data/blood_cell_cancer_detection')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "OUTPUT_JSONL = DATA_DIR / 'blood_cell_images.jsonl'\n",
    "\n",
    "DATASET_SEARCH_TERM = 'Blood Cell images for Cancer detection'\n",
    "DATASET_REF_OVERRIDE = None  # Set to the explicit Kaggle dataset ref if you already know it\n",
    "\n",
    "print(f'Work directory: {DATA_DIR.resolve()}')\n",
    "print(f'Output JSONL will be written to: {OUTPUT_JSONL.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9caf263",
   "metadata": {},
   "source": [
    "## 3. Validate Kaggle credentials\n",
    "Either place the `kaggle.json` file under `~/.kaggle/` or populate the `KAGGLE_USERNAME` and `KAGGLE_KEY` environment variables before running this cell.\n",
    "The location of the kaggle.json file is normally `<your user directory\\.kaggle\\>` e.g. something like `C:\\users\\<your windows user name>\\.kaggle\\` or within your home directory on Linux / MacOS.\n",
    "\n",
    "The content of kaggle.json file will be similar to this:\n",
    "`{\"username\":\"<your kaggle username>\",\"key\":\"<your API key from kaggle>\"}`\n",
    "\n",
    "If you don't have an API key from kaggle:\n",
    "- login to [kaggle](https://www.kaggle.com/) with your credentials (register if necessary)\n",
    "- click on your user name (icon on the upper right corner, next to the search bar) and select \"Settings\" from the menu (https://www.kaggle.com/settings)\n",
    "- scroll down to the API section and create a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b73b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "kaggle_credentials_path = kaggle_dir / 'kaggle.json'\n",
    "\n",
    "if kaggle_credentials_path.exists():\n",
    "    with kaggle_credentials_path.open('r', encoding='utf-8') as cred_file:\n",
    "        credentials = json.load(cred_file)\n",
    "    os.environ.setdefault('KAGGLE_USERNAME', credentials.get('username', ''))\n",
    "    os.environ.setdefault('KAGGLE_KEY', credentials.get('key', ''))\n",
    "    kaggle_credentials_path.chmod(0o600)\n",
    "    print(f'Loaded Kaggle credentials from {kaggle_credentials_path}')\n",
    "elif not (os.environ.get('KAGGLE_USERNAME') and os.environ.get('KAGGLE_KEY')):\n",
    "    raise RuntimeError('Kaggle credentials not found. Upload kaggle.json to ~/.kaggle or set KAGGLE_USERNAME/KAGGLE_KEY.')\n",
    "else:\n",
    "    print('Using Kaggle credentials from environment variables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487cffa",
   "metadata": {},
   "source": [
    "## 4. Locate the Kaggle dataset\n",
    "Authenticate with the Kaggle API, search by text, and lock in the dataset reference (`DATASET_REF`) the rest of the notebook will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "search_results = []\n",
    "DATASET_REF = None\n",
    "\n",
    "if DATASET_REF_OVERRIDE:\n",
    "    DATASET_REF = DATASET_REF_OVERRIDE\n",
    "    print(f\"Using manually specified dataset ref: {DATASET_REF}\")\n",
    "else:\n",
    "    search_results = api.dataset_list(search=DATASET_SEARCH_TERM)\n",
    "    if not search_results:\n",
    "        raise ValueError(f\"No Kaggle datasets matched the search term: {DATASET_SEARCH_TERM}\")\n",
    "    print(\"Candidate datasets:\")\n",
    "    for ds in search_results:\n",
    "        total_bytes = getattr(ds, \"totalBytes\", None)\n",
    "        if total_bytes is None:\n",
    "            total_bytes = getattr(ds, \"total_bytes\", None)\n",
    "        size_mb = (total_bytes or 0) / (1024 ** 2)\n",
    "        print(f\"  - {ds.ref} | {ds.title} | approx {size_mb:.2f} MB\")\n",
    "    DATASET_REF = search_results[0].ref\n",
    "    print(f\"Defaulting to the first match: {DATASET_REF}\")\n",
    "\n",
    "if not DATASET_REF:\n",
    "    raise ValueError(\"No dataset reference available; set DATASET_REF_OVERRIDE or adjust the search term.\")\n",
    "\n",
    "DATASET_SLUG = DATASET_REF.split('/')[-1]\n",
    "ZIP_PATH = DATA_DIR / f\"{DATASET_SLUG}.zip\"\n",
    "print(f\"Archive will be stored at: {ZIP_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdee1ce",
   "metadata": {},
   "source": [
    "## 5. Download the dataset archive\n",
    "Files are saved under the working directory; rerunning this cell is safe because it will skip the download if the zip already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ZIP_PATH.exists():\n",
    "    print(f'Skipping download because {ZIP_PATH.name} already exists.')\n",
    "else:\n",
    "    api.dataset_download_files(DATASET_REF, path=str(DATA_DIR), force=False, quiet=False, unzip=False)\n",
    "    if not ZIP_PATH.exists():\n",
    "        raise FileNotFoundError(f'Expected archive {ZIP_PATH} was not created; check Kaggle output above.')\n",
    "    print(f'Download complete: {ZIP_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3fa7b",
   "metadata": {},
   "source": [
    "## 6. Extract the raw images\n",
    "Start fresh each run by clearing the extraction directory before unzipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "if RAW_DIR.exists():\n",
    "    try:\n",
    "        shutil.rmtree(RAW_DIR)\n",
    "    except PermissionError as exc:\n",
    "        print(f\"Skipping RAW_DIR cleanup because it is locked ({exc}). Existing files may be reused.\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(ZIP_PATH, 'r') as archive:\n",
    "    archive.extractall(RAW_DIR)\n",
    "\n",
    "file_count = sum(1 for _ in RAW_DIR.rglob('*') if _.is_file())\n",
    "print(f'Extracted {file_count} files into {RAW_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b462e",
   "metadata": {},
   "source": [
    "## 7. Convert images to a JSONL corpus\n",
    "Each record will include: a UUID, Kaggle dataset ref, relative path, filename, inferred label (parent directory name), image dimensions/mode/format, and a base64-encoded payload of the exact bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from uuid import uuid4\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n",
    "format_to_mime = {\n",
    "    'JPEG': 'image/jpeg',\n",
    "    'JPG': 'image/jpeg',\n",
    "    'PNG': 'image/png',\n",
    "    'BMP': 'image/bmp',\n",
    "    'TIFF': 'image/tiff',\n",
    "    'TIF': 'image/tiff',\n",
    "}\n",
    "extension_to_mime = {\n",
    "    '.jpg': 'image/jpeg',\n",
    "    '.jpeg': 'image/jpeg',\n",
    "    '.png': 'image/png',\n",
    "    '.bmp': 'image/bmp',\n",
    "    '.tif': 'image/tiff',\n",
    "    '.tiff': 'image/tiff',\n",
    "}\n",
    "\n",
    "def infer_mime(image_format: str | None, suffix: str) -> str:\n",
    "    if image_format:\n",
    "        mime = format_to_mime.get(image_format.upper())\n",
    "        if mime:\n",
    "            return mime\n",
    "    return extension_to_mime.get(suffix.lower(), 'application/octet-stream')\n",
    "\n",
    "image_paths = sorted(p for p in RAW_DIR.rglob('*') if p.suffix.lower() in valid_extensions)\n",
    "\n",
    "if not image_paths:\n",
    "    raise RuntimeError(f'No images found under {RAW_DIR}. Please inspect the extraction output and update the parsing logic if needed.')\n",
    "\n",
    "with OUTPUT_JSONL.open('w', encoding='utf-8') as writer:\n",
    "    for image_path in tqdm(image_paths, desc='Encoding images'):\n",
    "        relative_path = image_path.relative_to(RAW_DIR)\n",
    "        label = image_path.parent.name\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "                color_mode = img.mode\n",
    "                image_format = img.format\n",
    "        except Exception as exc:\n",
    "            width = height = None\n",
    "            color_mode = image_format = None\n",
    "            print(f'Warning: failed to read metadata for {image_path}: {exc}')\n",
    "        with image_path.open('rb') as img_file:\n",
    "            payload_b64 = base64.b64encode(img_file.read()).decode('ascii')\n",
    "        mime_type = infer_mime(image_format, image_path.suffix)\n",
    "        data_uri = f\"data:{mime_type};base64,{payload_b64}\"\n",
    "        record = {\n",
    "            'id': str(uuid4()),\n",
    "            'dataset_ref': DATASET_REF,\n",
    "            'relative_path': relative_path.as_posix(),\n",
    "            'filename': image_path.name,\n",
    "            'label': label,\n",
    "            'width': width,\n",
    "            'height': height,\n",
    "            'color_mode': color_mode,\n",
    "            'image_format': image_format,\n",
    "            'label_source': 'parent_directory_name',\n",
    "            'images': [data_uri],\n",
    "        }\n",
    "        writer.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f'Wrote {len(image_paths)} image records to {OUTPUT_JSONL}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a748e",
   "metadata": {},
   "source": [
    "## 8. Inspect the JSONL output\n",
    "Confirm the number of records, label distribution, and preview a sample entry (with the base64 payload truncated for readability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cebc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_records = 0\n",
    "label_counts = Counter()\n",
    "sample_record = None\n",
    "\n",
    "with OUTPUT_JSONL.open('r', encoding='utf-8') as reader:\n",
    "    for line in reader:\n",
    "        total_records += 1\n",
    "        record = json.loads(line)\n",
    "        label_counts[record['label']] += 1\n",
    "        if sample_record is None:\n",
    "            sample_record = record\n",
    "\n",
    "if sample_record:\n",
    "    sample_record = dict(sample_record)\n",
    "    if sample_record.get('images'):\n",
    "        sample_record['images'][0] = sample_record['images'][0][:80] + '...'\n",
    "\n",
    "print(f'Total records: {total_records}')\n",
    "print('Label distribution:')\n",
    "for label, count in label_counts.most_common():\n",
    "    print(f'  {label}: {count}')\n",
    "print('\\nSample record:')\n",
    "print(sample_record)\n",
    "print(f'JSONL location: {OUTPUT_JSONL.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790d8b5",
   "metadata": {},
   "source": [
    "âœ… All steps completed. You now have a single JSON Lines corpus with base64-embedded blood cell images plus metadata/labels derived from the folder structure. \n",
    "The jsonl file is in this location: `../data/blood_cell_cancer_detection`\n",
    "\n",
    "DOI (Digital Object Identifier)\n",
    "https://doi.org/10.34740/kaggle/dsv/10500753\n",
    "\n",
    "@misc{sumith_singh_kothwal_2025,\n",
    "\ttitle={Blood Cell images for Cancer detection},\n",
    "\turl={https://www.kaggle.com/dsv/10500753},\n",
    "\tDOI={10.34740/KAGGLE/DSV/10500753},\n",
    "\tpublisher={Kaggle},\n",
    "\tauthor={Sumith Singh Kothwal},\n",
    "\tyear={2025}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e986557",
   "metadata": {},
   "source": [
    "Now we will create a small subset from this large dataset for testing purposes. We will pick every 237th (configurable) line from the jsonl file and add them to the 2nd jsonl file with _subset suffix. \n",
    "This will create: data\\blood_cell_cancer_detection\\blood_cell_images_subset.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SUBSET_STRIDE = 237 # Adjust this value to change the sampling rate. There are 5000 lines in the full file. 237 gives ~21 lines.\n",
    "subset_path = OUTPUT_JSONL.with_stem(OUTPUT_JSONL.stem + '_subset')\n",
    "subset_path = subset_path.with_suffix('.jsonl')\n",
    "\n",
    "selected = 0\n",
    "total = 0\n",
    "\n",
    "with OUTPUT_JSONL.open('r', encoding='utf-8') as source, subset_path.open('w', encoding='utf-8') as target:\n",
    "    for idx, line in enumerate(source):\n",
    "        total += 1\n",
    "        if idx % SUBSET_STRIDE == 0:\n",
    "            target.write(line)\n",
    "            selected += 1\n",
    "\n",
    "print(f'Saved every {SUBSET_STRIDE}th record to {subset_path}')\n",
    "print(f'Total source records: {total}')\n",
    "print(f'Total subset records: {selected}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
